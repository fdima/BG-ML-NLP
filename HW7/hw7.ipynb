{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.6"},"colab":{"name":"hw7.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"jZPYjo1Ic9b6","executionInfo":{"status":"ok","timestamp":1606201242319,"user_tz":-180,"elapsed":2054,"user":{"displayName":"Дмитрий Фукин","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhOgXORSMlpkMpukDZcLijzceS9AaqgT1tfLKD3jA=s64","userId":"04706038827385634385"}}},"source":["from keras.models import Model\n","from keras.layers import Input, LSTM, Dense\n","import numpy as np"],"execution_count":1,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6GTLgdr3ZEEb","executionInfo":{"status":"ok","timestamp":1606201261443,"user_tz":-180,"elapsed":21155,"user":{"displayName":"Дмитрий Фукин","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhOgXORSMlpkMpukDZcLijzceS9AaqgT1tfLKD3jA=s64","userId":"04706038827385634385"}},"outputId":"c70185b6-b194-449f-9481-32e805354b88"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"c7n1fxUGc9cA","executionInfo":{"status":"ok","timestamp":1606201261445,"user_tz":-180,"elapsed":21154,"user":{"displayName":"Дмитрий Фукин","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhOgXORSMlpkMpukDZcLijzceS9AaqgT1tfLKD3jA=s64","userId":"04706038827385634385"}}},"source":["batch_size = 64\n","epochs = 100\n","latent_dim = 256\n","num_samples = 25000\n","data_path = '/content/drive/MyDrive/NLP/HW/data/rus-eng/rus.txt'"],"execution_count":3,"outputs":[]},{"cell_type":"code","metadata":{"id":"HZsZmEgYAIwH","executionInfo":{"status":"ok","timestamp":1606201261446,"user_tz":-180,"elapsed":21153,"user":{"displayName":"Дмитрий Фукин","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhOgXORSMlpkMpukDZcLijzceS9AaqgT1tfLKD3jA=s64","userId":"04706038827385634385"}}},"source":["input_texts = []\n","target_texts = []\n","input_characters = set()\n","target_characters = set()"],"execution_count":4,"outputs":[]},{"cell_type":"code","metadata":{"id":"q6-PIOIFc9cE","executionInfo":{"status":"ok","timestamp":1606201263822,"user_tz":-180,"elapsed":23527,"user":{"displayName":"Дмитрий Фукин","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhOgXORSMlpkMpukDZcLijzceS9AaqgT1tfLKD3jA=s64","userId":"04706038827385634385"}}},"source":["# Собираем из текстов токены и делаем pne-hot вектора на каждый токен\n","with open(data_path, 'r', encoding='utf-8') as f:\n","    lines = f.read().split('\\n')\n","    \n","for line in lines[: min(num_samples, len(lines) - 1)]:\n","    input_text, target_text, _ = line.split('\\t')\n","    target_text = '\\t' + target_text + '\\n'\n","    input_texts.append(input_text)\n","    target_texts.append(target_text)\n","    for char in input_text:\n","        if char not in input_characters:\n","            input_characters.add(char)\n","    for char in target_text:\n","        if char not in target_characters:\n","            target_characters.add(char)\n","\n","input_characters = sorted(list(input_characters))\n","target_characters = sorted(list(target_characters))\n","num_encoder_tokens = len(input_characters)\n","num_decoder_tokens = len(target_characters)\n","max_encoder_seq_length = max([len(txt) for txt in input_texts])\n","max_decoder_seq_length = max([len(txt) for txt in target_texts])"],"execution_count":5,"outputs":[]},{"cell_type":"code","metadata":{"id":"18sZb83cc9cK","executionInfo":{"status":"ok","timestamp":1606201263828,"user_tz":-180,"elapsed":23532,"user":{"displayName":"Дмитрий Фукин","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhOgXORSMlpkMpukDZcLijzceS9AaqgT1tfLKD3jA=s64","userId":"04706038827385634385"}}},"source":["input_token_index = dict(\n","    [(char, i) for i, char in enumerate(input_characters)])\n","target_token_index = dict(\n","    [(char, i) for i, char in enumerate(target_characters)])"],"execution_count":6,"outputs":[]},{"cell_type":"code","metadata":{"id":"50H5TRq2c9cP","executionInfo":{"status":"ok","timestamp":1606201264248,"user_tz":-180,"elapsed":23949,"user":{"displayName":"Дмитрий Фукин","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhOgXORSMlpkMpukDZcLijzceS9AaqgT1tfLKD3jA=s64","userId":"04706038827385634385"}}},"source":["encoder_input_data = np.zeros((len(input_texts), max_encoder_seq_length, num_encoder_tokens), dtype='float32')\n","decoder_input_data = np.zeros((len(input_texts), max_decoder_seq_length, num_decoder_tokens), dtype='float32')\n","decoder_target_data = np.zeros((len(input_texts), max_decoder_seq_length, num_decoder_tokens), dtype='float32')"],"execution_count":7,"outputs":[]},{"cell_type":"code","metadata":{"id":"G-lW6UmJc9cT","executionInfo":{"status":"ok","timestamp":1606201264917,"user_tz":-180,"elapsed":24615,"user":{"displayName":"Дмитрий Фукин","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhOgXORSMlpkMpukDZcLijzceS9AaqgT1tfLKD3jA=s64","userId":"04706038827385634385"}}},"source":["for i, (input_text, target_text) in enumerate(zip(input_texts, target_texts)):\n","    for t, char in enumerate(input_text):\n","        encoder_input_data[i, t, input_token_index[char]] = 1.\n","    encoder_input_data[i, t + 1:, input_token_index[' ']] = 1.\n","    for t, char in enumerate(target_text):\n","        decoder_input_data[i, t, target_token_index[char]] = 1.\n","        if t > 0:\n","            decoder_target_data[i, t - 1, target_token_index[char]] = 1.\n","    decoder_input_data[i, t + 1:, target_token_index[' ']] = 1.\n","    decoder_target_data[i, t:, target_token_index[' ']] = 1."],"execution_count":8,"outputs":[]},{"cell_type":"code","metadata":{"id":"1rKfQBJTAYbV","executionInfo":{"status":"ok","timestamp":1606201264921,"user_tz":-180,"elapsed":24617,"user":{"displayName":"Дмитрий Фукин","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhOgXORSMlpkMpukDZcLijzceS9AaqgT1tfLKD3jA=s64","userId":"04706038827385634385"}}},"source":["def decode_sequence(input_seq):\n","    states_value = encoder_model.predict(input_seq)\n","\n","    target_seq = np.zeros((1, 1, num_decoder_tokens))\n","    target_seq[0, 0, target_token_index['\\t']] = 1.\n","\n","    stop_condition = False\n","    decoded_sentence = ''\n","    while not stop_condition:\n","        output_tokens, h, c = decoder_model.predict(\n","            [target_seq] + states_value)\n","\n","        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n","        sampled_char = reverse_target_char_index[sampled_token_index]\n","        decoded_sentence += sampled_char\n","\n","        if (sampled_char == '\\n' or\n","           len(decoded_sentence) > max_decoder_seq_length):\n","            stop_condition = True\n","        target_seq = np.zeros((1, 1, num_decoder_tokens))\n","        target_seq[0, 0, sampled_token_index] = 1.\n","\n","        states_value = [h, c]\n","\n","    return decoded_sentence"],"execution_count":9,"outputs":[]},{"cell_type":"code","metadata":{"id":"EGpCwPoOc9cX","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1606201710395,"user_tz":-180,"elapsed":470084,"user":{"displayName":"Дмитрий Фукин","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhOgXORSMlpkMpukDZcLijzceS9AaqgT1tfLKD3jA=s64","userId":"04706038827385634385"}},"outputId":"e04b0ee4-8387-493e-c39a-164848272607"},"source":["%%time\n","encoder_inputs = Input(shape=(None, num_encoder_tokens))\n","encoder = LSTM(latent_dim, return_state=True)\n","encoder_outputs, state_h, state_c = encoder(encoder_inputs)\n","encoder_states = [state_h, state_c]\n","\n","decoder_inputs = Input(shape=(None, num_decoder_tokens))\n","decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True)\n","decoder_outputs, _, _ = decoder_lstm(decoder_inputs,\n","                                     initial_state=encoder_states)\n","decoder_dense = Dense(num_decoder_tokens, activation='softmax')\n","decoder_outputs = decoder_dense(decoder_outputs)\n","\n","model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n","\n","model.compile(optimizer='adam', loss='categorical_crossentropy',\n","              metrics=['accuracy'])\n","model.fit([encoder_input_data, decoder_input_data], decoder_target_data,\n","          batch_size=batch_size,\n","          epochs=epochs,\n","          validation_split=0.2)\n","\n","encoder_model = Model(encoder_inputs, encoder_states)\n","\n","decoder_state_input_h = Input(shape=(latent_dim,))\n","decoder_state_input_c = Input(shape=(latent_dim,))\n","decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n","decoder_outputs, state_h, state_c = decoder_lstm(\n","    decoder_inputs, initial_state=decoder_states_inputs)\n","decoder_states = [state_h, state_c]\n","decoder_outputs = decoder_dense(decoder_outputs)\n","decoder_model = Model(\n","    [decoder_inputs] + decoder_states_inputs,\n","    [decoder_outputs] + decoder_states)\n","\n","reverse_input_char_index = dict(\n","    (i, char) for char, i in input_token_index.items())\n","reverse_target_char_index = dict(\n","    (i, char) for char, i in target_token_index.items())\n","\n","for seq_index in range(100):\n","    input_seq = encoder_input_data[seq_index: seq_index + 1]\n","    decoded_sentence = decode_sequence(input_seq)\n","    print('-')\n","    print('Input sentence:', input_texts[seq_index])\n","    print('Decoded sentence:', decoded_sentence)"],"execution_count":10,"outputs":[{"output_type":"stream","text":["Epoch 1/100\n","313/313 [==============================] - 4s 14ms/step - loss: 0.9680 - accuracy: 0.7740 - val_loss: 0.8375 - val_accuracy: 0.7801\n","Epoch 2/100\n","313/313 [==============================] - 4s 12ms/step - loss: 0.6279 - accuracy: 0.8271 - val_loss: 0.6802 - val_accuracy: 0.8068\n","Epoch 3/100\n","313/313 [==============================] - 4s 12ms/step - loss: 0.6278 - accuracy: 0.8286 - val_loss: 0.6669 - val_accuracy: 0.8102\n","Epoch 4/100\n","313/313 [==============================] - 4s 12ms/step - loss: 0.5484 - accuracy: 0.8409 - val_loss: 0.6347 - val_accuracy: 0.8169\n","Epoch 5/100\n","313/313 [==============================] - 4s 12ms/step - loss: 0.5209 - accuracy: 0.8478 - val_loss: 0.6138 - val_accuracy: 0.8218\n","Epoch 6/100\n","313/313 [==============================] - 4s 12ms/step - loss: 0.4989 - accuracy: 0.8540 - val_loss: 0.5928 - val_accuracy: 0.8274\n","Epoch 7/100\n","313/313 [==============================] - 4s 12ms/step - loss: 0.4797 - accuracy: 0.8596 - val_loss: 0.5761 - val_accuracy: 0.8348\n","Epoch 8/100\n","313/313 [==============================] - 4s 12ms/step - loss: 0.4613 - accuracy: 0.8647 - val_loss: 0.5584 - val_accuracy: 0.8386\n","Epoch 9/100\n","313/313 [==============================] - 4s 13ms/step - loss: 0.4452 - accuracy: 0.8693 - val_loss: 0.5421 - val_accuracy: 0.8437\n","Epoch 10/100\n","313/313 [==============================] - 4s 12ms/step - loss: 0.4300 - accuracy: 0.8740 - val_loss: 0.5297 - val_accuracy: 0.8482\n","Epoch 11/100\n","313/313 [==============================] - 4s 12ms/step - loss: 0.4153 - accuracy: 0.8785 - val_loss: 0.5159 - val_accuracy: 0.8520\n","Epoch 12/100\n","313/313 [==============================] - 4s 12ms/step - loss: 0.4010 - accuracy: 0.8828 - val_loss: 0.5038 - val_accuracy: 0.8560\n","Epoch 13/100\n","313/313 [==============================] - 4s 12ms/step - loss: 0.3875 - accuracy: 0.8867 - val_loss: 0.4903 - val_accuracy: 0.8592\n","Epoch 14/100\n","313/313 [==============================] - 4s 12ms/step - loss: 0.3744 - accuracy: 0.8905 - val_loss: 0.4811 - val_accuracy: 0.8625\n","Epoch 15/100\n","313/313 [==============================] - 4s 12ms/step - loss: 0.3616 - accuracy: 0.8943 - val_loss: 0.4724 - val_accuracy: 0.8655\n","Epoch 16/100\n","313/313 [==============================] - 4s 12ms/step - loss: 0.3502 - accuracy: 0.8978 - val_loss: 0.4633 - val_accuracy: 0.8690\n","Epoch 17/100\n","313/313 [==============================] - 4s 12ms/step - loss: 0.3386 - accuracy: 0.9009 - val_loss: 0.4519 - val_accuracy: 0.8714\n","Epoch 18/100\n","313/313 [==============================] - 4s 12ms/step - loss: 0.3281 - accuracy: 0.9036 - val_loss: 0.4475 - val_accuracy: 0.8727\n","Epoch 19/100\n","313/313 [==============================] - 4s 12ms/step - loss: 0.3179 - accuracy: 0.9065 - val_loss: 0.4376 - val_accuracy: 0.8756\n","Epoch 20/100\n","313/313 [==============================] - 4s 12ms/step - loss: 0.3086 - accuracy: 0.9092 - val_loss: 0.4316 - val_accuracy: 0.8775\n","Epoch 21/100\n","313/313 [==============================] - 4s 12ms/step - loss: 0.3047 - accuracy: 0.9102 - val_loss: 0.4295 - val_accuracy: 0.8783\n","Epoch 22/100\n","313/313 [==============================] - 4s 12ms/step - loss: 0.2915 - accuracy: 0.9140 - val_loss: 0.4237 - val_accuracy: 0.8799\n","Epoch 23/100\n","313/313 [==============================] - 4s 12ms/step - loss: 0.2819 - accuracy: 0.9168 - val_loss: 0.4194 - val_accuracy: 0.8811\n","Epoch 24/100\n","313/313 [==============================] - 4s 12ms/step - loss: 0.2735 - accuracy: 0.9193 - val_loss: 0.4134 - val_accuracy: 0.8829\n","Epoch 25/100\n","313/313 [==============================] - 4s 12ms/step - loss: 0.2657 - accuracy: 0.9215 - val_loss: 0.4112 - val_accuracy: 0.8838\n","Epoch 26/100\n","313/313 [==============================] - 4s 12ms/step - loss: 0.2579 - accuracy: 0.9237 - val_loss: 0.4074 - val_accuracy: 0.8851\n","Epoch 27/100\n","313/313 [==============================] - 4s 12ms/step - loss: 0.2501 - accuracy: 0.9260 - val_loss: 0.4066 - val_accuracy: 0.8854\n","Epoch 28/100\n","313/313 [==============================] - 4s 12ms/step - loss: 0.2430 - accuracy: 0.9280 - val_loss: 0.4066 - val_accuracy: 0.8860\n","Epoch 29/100\n","313/313 [==============================] - 4s 12ms/step - loss: 0.2361 - accuracy: 0.9299 - val_loss: 0.4045 - val_accuracy: 0.8871\n","Epoch 30/100\n","313/313 [==============================] - 4s 12ms/step - loss: 0.2291 - accuracy: 0.9319 - val_loss: 0.4024 - val_accuracy: 0.8880\n","Epoch 31/100\n","313/313 [==============================] - 4s 12ms/step - loss: 0.2226 - accuracy: 0.9337 - val_loss: 0.3986 - val_accuracy: 0.8895\n","Epoch 32/100\n","313/313 [==============================] - 4s 12ms/step - loss: 0.2157 - accuracy: 0.9358 - val_loss: 0.4076 - val_accuracy: 0.8875\n","Epoch 33/100\n","313/313 [==============================] - 4s 12ms/step - loss: 0.2104 - accuracy: 0.9371 - val_loss: 0.4021 - val_accuracy: 0.8892\n","Epoch 34/100\n","313/313 [==============================] - 4s 13ms/step - loss: 0.2039 - accuracy: 0.9390 - val_loss: 0.4043 - val_accuracy: 0.8896\n","Epoch 35/100\n","313/313 [==============================] - 4s 13ms/step - loss: 0.1982 - accuracy: 0.9405 - val_loss: 0.4048 - val_accuracy: 0.8896\n","Epoch 36/100\n","313/313 [==============================] - 4s 12ms/step - loss: 0.1923 - accuracy: 0.9422 - val_loss: 0.4125 - val_accuracy: 0.8887\n","Epoch 37/100\n","313/313 [==============================] - 4s 12ms/step - loss: 0.1870 - accuracy: 0.9435 - val_loss: 0.4058 - val_accuracy: 0.8904\n","Epoch 38/100\n","313/313 [==============================] - 4s 12ms/step - loss: 0.1815 - accuracy: 0.9454 - val_loss: 0.4097 - val_accuracy: 0.8903\n","Epoch 39/100\n","313/313 [==============================] - 4s 12ms/step - loss: 0.1762 - accuracy: 0.9469 - val_loss: 0.4101 - val_accuracy: 0.8917\n","Epoch 40/100\n","313/313 [==============================] - 4s 12ms/step - loss: 0.1713 - accuracy: 0.9480 - val_loss: 0.4124 - val_accuracy: 0.8910\n","Epoch 41/100\n","313/313 [==============================] - 4s 12ms/step - loss: 0.1660 - accuracy: 0.9498 - val_loss: 0.4173 - val_accuracy: 0.8904\n","Epoch 42/100\n","313/313 [==============================] - 4s 12ms/step - loss: 0.1613 - accuracy: 0.9510 - val_loss: 0.4200 - val_accuracy: 0.8907\n","Epoch 43/100\n","313/313 [==============================] - 4s 12ms/step - loss: 0.1568 - accuracy: 0.9523 - val_loss: 0.4224 - val_accuracy: 0.8909\n","Epoch 44/100\n","313/313 [==============================] - 4s 12ms/step - loss: 0.1526 - accuracy: 0.9535 - val_loss: 0.4275 - val_accuracy: 0.8909\n","Epoch 45/100\n","313/313 [==============================] - 4s 13ms/step - loss: 0.1481 - accuracy: 0.9548 - val_loss: 0.4312 - val_accuracy: 0.8905\n","Epoch 46/100\n","313/313 [==============================] - 4s 13ms/step - loss: 0.1441 - accuracy: 0.9559 - val_loss: 0.4386 - val_accuracy: 0.8904\n","Epoch 47/100\n","313/313 [==============================] - 4s 12ms/step - loss: 0.1395 - accuracy: 0.9574 - val_loss: 0.4389 - val_accuracy: 0.8904\n","Epoch 48/100\n","313/313 [==============================] - 4s 13ms/step - loss: 0.1356 - accuracy: 0.9584 - val_loss: 0.4430 - val_accuracy: 0.8904\n","Epoch 49/100\n","313/313 [==============================] - 4s 13ms/step - loss: 0.1323 - accuracy: 0.9594 - val_loss: 0.4495 - val_accuracy: 0.8901\n","Epoch 50/100\n","313/313 [==============================] - 4s 12ms/step - loss: 0.1280 - accuracy: 0.9607 - val_loss: 0.4519 - val_accuracy: 0.8900\n","Epoch 51/100\n","313/313 [==============================] - 4s 12ms/step - loss: 0.1248 - accuracy: 0.9616 - val_loss: 0.4558 - val_accuracy: 0.8896\n","Epoch 52/100\n","313/313 [==============================] - 4s 12ms/step - loss: 0.1215 - accuracy: 0.9626 - val_loss: 0.4621 - val_accuracy: 0.8899\n","Epoch 53/100\n","313/313 [==============================] - 4s 12ms/step - loss: 0.1179 - accuracy: 0.9636 - val_loss: 0.4689 - val_accuracy: 0.8896\n","Epoch 54/100\n","313/313 [==============================] - 4s 12ms/step - loss: 0.1151 - accuracy: 0.9642 - val_loss: 0.4691 - val_accuracy: 0.8895\n","Epoch 55/100\n","313/313 [==============================] - 4s 13ms/step - loss: 0.1118 - accuracy: 0.9654 - val_loss: 0.4823 - val_accuracy: 0.8885\n","Epoch 56/100\n","313/313 [==============================] - 4s 13ms/step - loss: 0.1090 - accuracy: 0.9662 - val_loss: 0.4816 - val_accuracy: 0.8886\n","Epoch 57/100\n","313/313 [==============================] - 4s 12ms/step - loss: 0.1059 - accuracy: 0.9670 - val_loss: 0.4896 - val_accuracy: 0.8881\n","Epoch 58/100\n","313/313 [==============================] - 4s 12ms/step - loss: 0.1032 - accuracy: 0.9678 - val_loss: 0.4932 - val_accuracy: 0.8883\n","Epoch 59/100\n","313/313 [==============================] - 4s 12ms/step - loss: 0.1003 - accuracy: 0.9687 - val_loss: 0.4996 - val_accuracy: 0.8880\n","Epoch 60/100\n","313/313 [==============================] - 4s 13ms/step - loss: 0.0975 - accuracy: 0.9694 - val_loss: 0.5025 - val_accuracy: 0.8879\n","Epoch 61/100\n","313/313 [==============================] - 4s 13ms/step - loss: 0.0952 - accuracy: 0.9701 - val_loss: 0.5100 - val_accuracy: 0.8881\n","Epoch 62/100\n","313/313 [==============================] - 4s 12ms/step - loss: 0.0929 - accuracy: 0.9706 - val_loss: 0.5170 - val_accuracy: 0.8870\n","Epoch 63/100\n","313/313 [==============================] - 4s 12ms/step - loss: 0.0903 - accuracy: 0.9713 - val_loss: 0.5146 - val_accuracy: 0.8883\n","Epoch 64/100\n","313/313 [==============================] - 4s 13ms/step - loss: 0.0879 - accuracy: 0.9721 - val_loss: 0.5269 - val_accuracy: 0.8867\n","Epoch 65/100\n","313/313 [==============================] - 4s 13ms/step - loss: 0.0860 - accuracy: 0.9726 - val_loss: 0.5311 - val_accuracy: 0.8869\n","Epoch 66/100\n","313/313 [==============================] - 4s 12ms/step - loss: 0.0839 - accuracy: 0.9732 - val_loss: 0.5385 - val_accuracy: 0.8863\n","Epoch 67/100\n","313/313 [==============================] - 4s 12ms/step - loss: 0.0821 - accuracy: 0.9737 - val_loss: 0.5413 - val_accuracy: 0.8865\n","Epoch 68/100\n","313/313 [==============================] - 4s 12ms/step - loss: 0.0797 - accuracy: 0.9745 - val_loss: 0.5427 - val_accuracy: 0.8863\n","Epoch 69/100\n","313/313 [==============================] - 4s 12ms/step - loss: 0.0777 - accuracy: 0.9752 - val_loss: 0.5499 - val_accuracy: 0.8866\n","Epoch 70/100\n","313/313 [==============================] - 4s 12ms/step - loss: 0.0764 - accuracy: 0.9753 - val_loss: 0.5506 - val_accuracy: 0.8871\n","Epoch 71/100\n","313/313 [==============================] - 4s 12ms/step - loss: 0.0746 - accuracy: 0.9757 - val_loss: 0.5632 - val_accuracy: 0.8859\n","Epoch 72/100\n","313/313 [==============================] - 4s 12ms/step - loss: 0.0732 - accuracy: 0.9762 - val_loss: 0.5671 - val_accuracy: 0.8859\n","Epoch 73/100\n","313/313 [==============================] - 4s 12ms/step - loss: 0.0717 - accuracy: 0.9765 - val_loss: 0.5741 - val_accuracy: 0.8851\n","Epoch 74/100\n","313/313 [==============================] - 4s 13ms/step - loss: 0.0700 - accuracy: 0.9769 - val_loss: 0.5784 - val_accuracy: 0.8850\n","Epoch 75/100\n","313/313 [==============================] - 4s 12ms/step - loss: 0.0681 - accuracy: 0.9776 - val_loss: 0.5807 - val_accuracy: 0.8850\n","Epoch 76/100\n","313/313 [==============================] - 4s 12ms/step - loss: 0.0671 - accuracy: 0.9779 - val_loss: 0.5839 - val_accuracy: 0.8858\n","Epoch 77/100\n","313/313 [==============================] - 4s 13ms/step - loss: 0.0654 - accuracy: 0.9784 - val_loss: 0.5900 - val_accuracy: 0.8849\n","Epoch 78/100\n","313/313 [==============================] - 4s 13ms/step - loss: 0.0647 - accuracy: 0.9786 - val_loss: 0.5951 - val_accuracy: 0.8847\n","Epoch 79/100\n","313/313 [==============================] - 4s 13ms/step - loss: 0.0638 - accuracy: 0.9788 - val_loss: 0.5960 - val_accuracy: 0.8851\n","Epoch 80/100\n","313/313 [==============================] - 4s 13ms/step - loss: 0.0626 - accuracy: 0.9791 - val_loss: 0.6019 - val_accuracy: 0.8848\n","Epoch 81/100\n","313/313 [==============================] - 4s 12ms/step - loss: 0.0603 - accuracy: 0.9799 - val_loss: 0.6088 - val_accuracy: 0.8844\n","Epoch 82/100\n","313/313 [==============================] - 4s 12ms/step - loss: 0.0595 - accuracy: 0.9800 - val_loss: 0.6126 - val_accuracy: 0.8847\n","Epoch 83/100\n","313/313 [==============================] - 4s 13ms/step - loss: 0.0595 - accuracy: 0.9800 - val_loss: 0.6218 - val_accuracy: 0.8834\n","Epoch 84/100\n","313/313 [==============================] - 4s 12ms/step - loss: 0.0582 - accuracy: 0.9803 - val_loss: 0.6239 - val_accuracy: 0.8842\n","Epoch 85/100\n","313/313 [==============================] - 4s 13ms/step - loss: 0.0564 - accuracy: 0.9809 - val_loss: 0.6248 - val_accuracy: 0.8844\n","Epoch 86/100\n","313/313 [==============================] - 4s 12ms/step - loss: 0.0556 - accuracy: 0.9812 - val_loss: 0.6308 - val_accuracy: 0.8839\n","Epoch 87/100\n","313/313 [==============================] - 4s 13ms/step - loss: 0.0549 - accuracy: 0.9813 - val_loss: 0.6339 - val_accuracy: 0.8839\n","Epoch 88/100\n","313/313 [==============================] - 4s 13ms/step - loss: 0.0539 - accuracy: 0.9815 - val_loss: 0.6369 - val_accuracy: 0.8844\n","Epoch 89/100\n","313/313 [==============================] - 4s 13ms/step - loss: 0.0538 - accuracy: 0.9815 - val_loss: 0.6429 - val_accuracy: 0.8841\n","Epoch 90/100\n","313/313 [==============================] - 4s 13ms/step - loss: 0.0524 - accuracy: 0.9821 - val_loss: 0.6489 - val_accuracy: 0.8832\n","Epoch 91/100\n","313/313 [==============================] - 4s 12ms/step - loss: 0.0521 - accuracy: 0.9821 - val_loss: 0.6548 - val_accuracy: 0.8829\n","Epoch 92/100\n","313/313 [==============================] - 4s 13ms/step - loss: 0.0507 - accuracy: 0.9825 - val_loss: 0.6543 - val_accuracy: 0.8838\n","Epoch 93/100\n","313/313 [==============================] - 4s 12ms/step - loss: 0.0503 - accuracy: 0.9827 - val_loss: 0.6536 - val_accuracy: 0.8840\n","Epoch 94/100\n","313/313 [==============================] - 4s 13ms/step - loss: 0.0498 - accuracy: 0.9828 - val_loss: 0.6575 - val_accuracy: 0.8832\n","Epoch 95/100\n","313/313 [==============================] - 4s 13ms/step - loss: 0.0491 - accuracy: 0.9830 - val_loss: 0.6668 - val_accuracy: 0.8826\n","Epoch 96/100\n","313/313 [==============================] - 4s 12ms/step - loss: 0.0486 - accuracy: 0.9830 - val_loss: 0.6651 - val_accuracy: 0.8838\n","Epoch 97/100\n","313/313 [==============================] - 4s 13ms/step - loss: 0.0478 - accuracy: 0.9833 - val_loss: 0.6723 - val_accuracy: 0.8832\n","Epoch 98/100\n","313/313 [==============================] - 4s 13ms/step - loss: 0.0476 - accuracy: 0.9834 - val_loss: 0.6796 - val_accuracy: 0.8827\n","Epoch 99/100\n","313/313 [==============================] - 4s 13ms/step - loss: 0.0467 - accuracy: 0.9836 - val_loss: 0.6753 - val_accuracy: 0.8835\n","Epoch 100/100\n","313/313 [==============================] - 4s 12ms/step - loss: 0.0457 - accuracy: 0.9839 - val_loss: 0.6804 - val_accuracy: 0.8832\n","-\n","Input sentence: Go.\n","Decoded sentence: Иди.\n","\n","-\n","Input sentence: Go.\n","Decoded sentence: Иди.\n","\n","-\n","Input sentence: Go.\n","Decoded sentence: Иди.\n","\n","-\n","Input sentence: Hi.\n","Decoded sentence: Здрасте.\n","\n","-\n","Input sentence: Hi.\n","Decoded sentence: Здрасте.\n","\n","-\n","Input sentence: Hi.\n","Decoded sentence: Здрасте.\n","\n","-\n","Input sentence: Hi.\n","Decoded sentence: Здрасте.\n","\n","-\n","Input sentence: Hi.\n","Decoded sentence: Здрасте.\n","\n","-\n","Input sentence: Run!\n","Decoded sentence: Бегите!\n","\n","-\n","Input sentence: Run!\n","Decoded sentence: Бегите!\n","\n","-\n","Input sentence: Run.\n","Decoded sentence: Бегите!\n","\n","-\n","Input sentence: Run.\n","Decoded sentence: Бегите!\n","\n","-\n","Input sentence: Who?\n","Decoded sentence: Кто?\n","\n","-\n","Input sentence: Wow!\n","Decoded sentence: Вот это да!\n","\n","-\n","Input sentence: Wow!\n","Decoded sentence: Вот это да!\n","\n","-\n","Input sentence: Wow!\n","Decoded sentence: Вот это да!\n","\n","-\n","Input sentence: Wow!\n","Decoded sentence: Вот это да!\n","\n","-\n","Input sentence: Wow!\n","Decoded sentence: Вот это да!\n","\n","-\n","Input sentence: Wow!\n","Decoded sentence: Вот это да!\n","\n","-\n","Input sentence: Fire!\n","Decoded sentence: Пожар!\n","\n","-\n","Input sentence: Fire!\n","Decoded sentence: Пожар!\n","\n","-\n","Input sentence: Help!\n","Decoded sentence: На помощь!\n","\n","-\n","Input sentence: Help!\n","Decoded sentence: На помощь!\n","\n","-\n","Input sentence: Help!\n","Decoded sentence: На помощь!\n","\n","-\n","Input sentence: Jump!\n","Decoded sentence: Прыгай!\n","\n","-\n","Input sentence: Jump!\n","Decoded sentence: Прыгай!\n","\n","-\n","Input sentence: Jump.\n","Decoded sentence: Прыгай!\n","\n","-\n","Input sentence: Jump.\n","Decoded sentence: Прыгай!\n","\n","-\n","Input sentence: Stop!\n","Decoded sentence: Остановись!\n","\n","-\n","Input sentence: Stop!\n","Decoded sentence: Остановись!\n","\n","-\n","Input sentence: Stop!\n","Decoded sentence: Остановись!\n","\n","-\n","Input sentence: Wait!\n","Decoded sentence: Жди!\n","\n","-\n","Input sentence: Wait!\n","Decoded sentence: Жди!\n","\n","-\n","Input sentence: Wait!\n","Decoded sentence: Жди!\n","\n","-\n","Input sentence: Wait!\n","Decoded sentence: Жди!\n","\n","-\n","Input sentence: Wait.\n","Decoded sentence: Ждите.\n","\n","-\n","Input sentence: Wait.\n","Decoded sentence: Ждите.\n","\n","-\n","Input sentence: Wait.\n","Decoded sentence: Ждите.\n","\n","-\n","Input sentence: Do it.\n","Decoded sentence: Сделай это.\n","\n","-\n","Input sentence: Go on.\n","Decoded sentence: Приходите огонь.\n","\n","-\n","Input sentence: Go on.\n","Decoded sentence: Приходите огонь.\n","\n","-\n","Input sentence: Hello!\n","Decoded sentence: Привет!\n","\n","-\n","Input sentence: Hello!\n","Decoded sentence: Привет!\n","\n","-\n","Input sentence: Hello!\n","Decoded sentence: Привет!\n","\n","-\n","Input sentence: Hello!\n","Decoded sentence: Привет!\n","\n","-\n","Input sentence: Hurry!\n","Decoded sentence: Поспешите.\n","\n","-\n","Input sentence: I ran.\n","Decoded sentence: Я бежал.\n","\n","-\n","Input sentence: I ran.\n","Decoded sentence: Я бежал.\n","\n","-\n","Input sentence: I ran.\n","Decoded sentence: Я бежал.\n","\n","-\n","Input sentence: I ran.\n","Decoded sentence: Я бежал.\n","\n","-\n","Input sentence: I see.\n","Decoded sentence: Понятно.\n","\n","-\n","Input sentence: I see.\n","Decoded sentence: Понятно.\n","\n","-\n","Input sentence: I see.\n","Decoded sentence: Понятно.\n","\n","-\n","Input sentence: I try.\n","Decoded sentence: Я пытаюсь.\n","\n","-\n","Input sentence: I try.\n","Decoded sentence: Я пытаюсь.\n","\n","-\n","Input sentence: I try.\n","Decoded sentence: Я пытаюсь.\n","\n","-\n","Input sentence: I won!\n","Decoded sentence: Я выиграл!\n","\n","-\n","Input sentence: I won!\n","Decoded sentence: Я выиграл!\n","\n","-\n","Input sentence: I won!\n","Decoded sentence: Я выиграл!\n","\n","-\n","Input sentence: I won!\n","Decoded sentence: Я выиграл!\n","\n","-\n","Input sentence: Oh no!\n","Decoded sentence: О нет!\n","\n","-\n","Input sentence: Relax.\n","Decoded sentence: Расслабьтесь.\n","\n","-\n","Input sentence: Relax.\n","Decoded sentence: Расслабьтесь.\n","\n","-\n","Input sentence: Relax.\n","Decoded sentence: Расслабьтесь.\n","\n","-\n","Input sentence: Shoot!\n","Decoded sentence: Потеряйся!\n","\n","-\n","Input sentence: Smile.\n","Decoded sentence: Улыбнитесь!\n","\n","-\n","Input sentence: Smile.\n","Decoded sentence: Улыбнитесь!\n","\n","-\n","Input sentence: Smile.\n","Decoded sentence: Улыбнитесь!\n","\n","-\n","Input sentence: Smile.\n","Decoded sentence: Улыбнитесь!\n","\n","-\n","Input sentence: Smile.\n","Decoded sentence: Улыбнитесь!\n","\n","-\n","Input sentence: Smile.\n","Decoded sentence: Улыбнитесь!\n","\n","-\n","Input sentence: Attack!\n","Decoded sentence: В атаку!\n","\n","-\n","Input sentence: Cheers!\n","Decoded sentence: За Ваше здоровье!\n","\n","-\n","Input sentence: Cheers!\n","Decoded sentence: За Ваше здоровье!\n","\n","-\n","Input sentence: Cheers!\n","Decoded sentence: За Ваше здоровье!\n","\n","-\n","Input sentence: Cheers!\n","Decoded sentence: За Ваше здоровье!\n","\n","-\n","Input sentence: Cheers!\n","Decoded sentence: За Ваше здоровье!\n","\n","-\n","Input sentence: Eat it.\n","Decoded sentence: Съешь это.\n","\n","-\n","Input sentence: Eat up.\n","Decoded sentence: Доедай.\n","\n","-\n","Input sentence: Freeze!\n","Decoded sentence: Замри!\n","\n","-\n","Input sentence: Freeze!\n","Decoded sentence: Замри!\n","\n","-\n","Input sentence: Freeze!\n","Decoded sentence: Замри!\n","\n","-\n","Input sentence: Freeze!\n","Decoded sentence: Замри!\n","\n","-\n","Input sentence: Get up.\n","Decoded sentence: Поднимайся.\n","\n","-\n","Input sentence: Get up.\n","Decoded sentence: Поднимайся.\n","\n","-\n","Input sentence: Get up.\n","Decoded sentence: Поднимайся.\n","\n","-\n","Input sentence: Go now.\n","Decoded sentence: Поезжай сейчас.\n","\n","-\n","Input sentence: Go now.\n","Decoded sentence: Поезжай сейчас.\n","\n","-\n","Input sentence: Go now.\n","Decoded sentence: Поезжай сейчас.\n","\n","-\n","Input sentence: Go now.\n","Decoded sentence: Поезжай сейчас.\n","\n","-\n","Input sentence: Go now.\n","Decoded sentence: Поезжай сейчас.\n","\n","-\n","Input sentence: Go now.\n","Decoded sentence: Поезжай сейчас.\n","\n","-\n","Input sentence: Go now.\n","Decoded sentence: Поезжай сейчас.\n","\n","-\n","Input sentence: Go now.\n","Decoded sentence: Поезжай сейчас.\n","\n","-\n","Input sentence: Go now.\n","Decoded sentence: Поезжай сейчас.\n","\n","-\n","Input sentence: Got it!\n","Decoded sentence: Устань!\n","\n","-\n","Input sentence: Got it?\n","Decoded sentence: Усёк?\n","\n","-\n","Input sentence: Got it?\n","Decoded sentence: Усёк?\n","\n","-\n","Input sentence: Got it?\n","Decoded sentence: Усёк?\n","\n","-\n","Input sentence: Got it?\n","Decoded sentence: Усёк?\n","\n","CPU times: user 7min 42s, sys: 47.4 s, total: 8min 30s\n","Wall time: 7min 25s\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"iB_FwJ_tc9ce"},"source":["Есть проблемы:\n","- на длинных последовательностях результат будет не очень\n"]},{"cell_type":"code","metadata":{"id":"ilr1U6JuA5zD","executionInfo":{"status":"ok","timestamp":1606201710397,"user_tz":-180,"elapsed":470083,"user":{"displayName":"Дмитрий Фукин","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhOgXORSMlpkMpukDZcLijzceS9AaqgT1tfLKD3jA=s64","userId":"04706038827385634385"}}},"source":["def preprocess_sentence(w):\n","    w = re.sub(r\"([?.!,¿])\", r\" \\1 \", w)\n","    w = re.sub(r'[\" \"]+', \" \", w)\n","    w = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", w)\n","    w = w.strip()\n","    w = '<start> ' + w + ' <end>'\n","    return w"],"execution_count":11,"outputs":[]},{"cell_type":"code","metadata":{"id":"O2MRFdLfc9ck","executionInfo":{"status":"ok","timestamp":1606201710397,"user_tz":-180,"elapsed":470080,"user":{"displayName":"Дмитрий Фукин","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhOgXORSMlpkMpukDZcLijzceS9AaqgT1tfLKD3jA=s64","userId":"04706038827385634385"}}},"source":["def tokenize(lang):\n","    lang_tokenizer = tf.keras.preprocessing.text.Tokenizer(\n","      filters='')\n","    lang_tokenizer.fit_on_texts(lang)\n","    tensor = lang_tokenizer.texts_to_sequences(lang)\n","    tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor,\n","                                                         padding='post')\n","    return tensor, lang_tokenizer"],"execution_count":12,"outputs":[]},{"cell_type":"code","metadata":{"id":"dEJPa13pc9cg","executionInfo":{"status":"ok","timestamp":1606201712209,"user_tz":-180,"elapsed":471889,"user":{"displayName":"Дмитрий Фукин","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhOgXORSMlpkMpukDZcLijzceS9AaqgT1tfLKD3jA=s64","userId":"04706038827385634385"}}},"source":["import re\n","import tensorflow.compat.v1 as tf\n","from sklearn.model_selection import train_test_split\n","data_path = '/content/drive/MyDrive/NLP/HW/data/rus-eng/rus.txt'\n","num_samples = 25000\n","\n","tf.enable_eager_execution()\n","\n","input_texts = []\n","target_texts = []\n","\n","with open(data_path, 'r', encoding='utf-8') as f:\n","    lines = f.read().split('\\n')\n","\n","for line in lines[: min(num_samples, len(lines) - 1)]:\n","    input_text, target_text, _ = line.split('\\t')\n","    target_text = '\\t' + target_text + '\\n'\n","    input_texts.append(preprocess_sentence(input_text))\n","    target_texts.append(preprocess_sentence(target_text))"],"execution_count":13,"outputs":[]},{"cell_type":"code","metadata":{"id":"P25UI3mDc9cn","executionInfo":{"status":"ok","timestamp":1606201712481,"user_tz":-180,"elapsed":472159,"user":{"displayName":"Дмитрий Фукин","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhOgXORSMlpkMpukDZcLijzceS9AaqgT1tfLKD3jA=s64","userId":"04706038827385634385"}}},"source":["input_tensor, inp_lang_tokenizer = tokenize(input_texts)\n","target_tensor, targ_lang_tokenizer = tokenize(target_texts)"],"execution_count":14,"outputs":[]},{"cell_type":"code","metadata":{"id":"RV3oMM6Oc9cq","executionInfo":{"status":"ok","timestamp":1606201712483,"user_tz":-180,"elapsed":472159,"user":{"displayName":"Дмитрий Фукин","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhOgXORSMlpkMpukDZcLijzceS9AaqgT1tfLKD3jA=s64","userId":"04706038827385634385"}}},"source":["input_tensor_train, input_tensor_val, target_tensor_train, target_tensor_val = train_test_split(input_tensor, target_tensor, test_size=0.2)"],"execution_count":15,"outputs":[]},{"cell_type":"code","metadata":{"id":"7WzPogGlc9cv","executionInfo":{"status":"ok","timestamp":1606201712485,"user_tz":-180,"elapsed":472160,"user":{"displayName":"Дмитрий Фукин","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhOgXORSMlpkMpukDZcLijzceS9AaqgT1tfLKD3jA=s64","userId":"04706038827385634385"}}},"source":["BUFFER_SIZE = len(input_tensor_train)\n","BATCH_SIZE = 64\n","steps_per_epoch = len(input_tensor_train)//BATCH_SIZE\n","embedding_dim = 256\n","units = 1024\n","\n","vocab_inp_size = len(inp_lang_tokenizer.word_index)+1\n","vocab_tar_size = len(targ_lang_tokenizer.word_index)+1\n","\n","dataset = tf.data.Dataset.from_tensor_slices((input_tensor_train, target_tensor_train)).shuffle(BUFFER_SIZE)\n","dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)"],"execution_count":16,"outputs":[]},{"cell_type":"code","metadata":{"id":"GK_YIl99c9cy","executionInfo":{"status":"ok","timestamp":1606201712488,"user_tz":-180,"elapsed":472161,"user":{"displayName":"Дмитрий Фукин","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhOgXORSMlpkMpukDZcLijzceS9AaqgT1tfLKD3jA=s64","userId":"04706038827385634385"}}},"source":["class Encoder(tf.keras.Model):\n","    def __init__(self, vocab_size, embedding_dim, enc_units, batch_sz):\n","        super(Encoder, self).__init__()\n","        self.batch_sz = batch_sz\n","        self.enc_units = enc_units\n","        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n","        self.lstm = tf.keras.layers.GRU(self.enc_units,\n","                                       return_sequences=True,\n","                                       return_state=True)\n","\n","    def call(self, x, hidden):\n","        x = self.embedding(x)\n","        output, state = self.lstm(x, initial_state = hidden)\n","        return output, state\n","\n","    def initialize_hidden_state(self):\n","        return tf.zeros((self.batch_sz, self.enc_units))\n","\n","    \n","class BahdanauAttention(tf.keras.layers.Layer):\n","    def __init__(self, units):\n","        super(BahdanauAttention, self).__init__()\n","        self.W1 = tf.keras.layers.Dense(units)\n","        self.W2 = tf.keras.layers.Dense(units)\n","        self.V = tf.keras.layers.Dense(1)\n","\n","    def call(self, query, values):\n","        query_with_time_axis = tf.expand_dims(query, 1)\n","        score = self.V(tf.nn.tanh(\n","            self.W1(query_with_time_axis) + self.W2(values)))\n","\n","        attention_weights = tf.nn.softmax(score, axis=1)\n","        context_vector = attention_weights * values\n","        context_vector = tf.reduce_sum(context_vector, axis=1)\n","\n","        return context_vector, attention_weights\n","    \n","    \n","class Decoder(tf.keras.Model):\n","    def __init__(self, vocab_size, embedding_dim, dec_units, batch_sz):\n","        super(Decoder, self).__init__()\n","        self.batch_sz = batch_sz\n","        self.dec_units = dec_units\n","        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n","        self.lstm = tf.keras.layers.GRU(self.dec_units,\n","                                       return_sequences=True,\n","                                       return_state=True)\n","        self.fc = tf.keras.layers.Dense(vocab_size)\n","\n","        self.attention = BahdanauAttention(self.dec_units)\n","\n","    def call(self, x, hidden, enc_output):\n","        context_vector, attention_weights = self.attention(hidden, enc_output)\n","        x = self.embedding(x)\n","        x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n","        output, state = self.lstm(x)\n","        output = tf.reshape(output, (-1, output.shape[2]))\n","        x = self.fc(output)\n","        return x, state, attention_weights"],"execution_count":17,"outputs":[]},{"cell_type":"code","metadata":{"id":"PabPBBVic9c1","executionInfo":{"status":"ok","timestamp":1606201712489,"user_tz":-180,"elapsed":472160,"user":{"displayName":"Дмитрий Фукин","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhOgXORSMlpkMpukDZcLijzceS9AaqgT1tfLKD3jA=s64","userId":"04706038827385634385"}}},"source":["encoder = Encoder(vocab_inp_size, embedding_dim, units, BATCH_SIZE)\n","decoder = Decoder(vocab_tar_size, embedding_dim, units, BATCH_SIZE)\n","\n","optimizer = tf.keras.optimizers.Adam()\n","loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n","    from_logits=True, reduction='none')\n","\n","def loss_function(real, pred):\n","    mask = tf.math.logical_not(tf.math.equal(real, 0))\n","    loss_ = loss_object(real, pred)\n","\n","    mask = tf.cast(mask, dtype=loss_.dtype)\n","    loss_ *= mask\n","\n","    return tf.reduce_mean(loss_)"],"execution_count":18,"outputs":[]},{"cell_type":"code","metadata":{"id":"24fjCYFGc9c4","executionInfo":{"status":"ok","timestamp":1606201712749,"user_tz":-180,"elapsed":472418,"user":{"displayName":"Дмитрий Фукин","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhOgXORSMlpkMpukDZcLijzceS9AaqgT1tfLKD3jA=s64","userId":"04706038827385634385"}}},"source":["@tf.function\n","def train_step(inp, targ, enc_hidden):\n","    loss = 0\n","\n","    with tf.GradientTape() as tape:\n","        enc_output, enc_hidden = encoder(inp, enc_hidden)\n","        dec_hidden = enc_hidden\n","        dec_input = tf.expand_dims([targ_lang_tokenizer.word_index['<start>']] * BATCH_SIZE, 1)\n","\n","        for t in range(1, targ.shape[1]):\n","            predictions, dec_hidden, _ = decoder(dec_input, dec_hidden, enc_output)\n","            loss += loss_function(targ[:, t], predictions)\n","            dec_input = tf.expand_dims(targ[:, t], 1)\n","    \n","    batch_loss = (loss / int(targ.shape[1]))\n","    variables = encoder.trainable_variables + decoder.trainable_variables\n","    gradients = tape.gradient(loss, variables)\n","    optimizer.apply_gradients(zip(gradients, variables))\n","    return batch_loss"],"execution_count":19,"outputs":[]},{"cell_type":"code","metadata":{"id":"QDTqHvPEc9c7","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1606203078042,"user_tz":-180,"elapsed":1837703,"user":{"displayName":"Дмитрий Фукин","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhOgXORSMlpkMpukDZcLijzceS9AaqgT1tfLKD3jA=s64","userId":"04706038827385634385"}},"outputId":"23b2bd73-24f0-4e40-e3a5-a18614149b05"},"source":["%%time\n","EPOCHS = 100\n","for epoch in range(EPOCHS):\n","    enc_hidden = encoder.initialize_hidden_state()\n","    total_loss = 0\n","\n","    for (batch, (inp, targ)) in enumerate(dataset.take(steps_per_epoch)):\n","        batch_loss = train_step(inp, targ, enc_hidden)\n","        total_loss += batch_loss\n","    \n","    print('Epoch {} Loss {:.4f}'.format(epoch + 1,\n","                                      total_loss / steps_per_epoch))"],"execution_count":20,"outputs":[{"output_type":"stream","text":["Epoch 1 Loss 0.0705\n","Epoch 2 Loss 0.0250\n","Epoch 3 Loss 0.0206\n","Epoch 4 Loss 0.0185\n","Epoch 5 Loss 0.0173\n","Epoch 6 Loss 0.0151\n","Epoch 7 Loss 0.0148\n","Epoch 8 Loss 0.0152\n","Epoch 9 Loss 0.0130\n","Epoch 10 Loss 0.0121\n","Epoch 11 Loss 0.0116\n","Epoch 12 Loss 0.0111\n","Epoch 13 Loss 0.0104\n","Epoch 14 Loss 0.0106\n","Epoch 15 Loss 0.0108\n","Epoch 16 Loss 0.0093\n","Epoch 17 Loss 0.0094\n","Epoch 18 Loss 0.0088\n","Epoch 19 Loss 0.0077\n","Epoch 20 Loss 0.0085\n","Epoch 21 Loss 0.0087\n","Epoch 22 Loss 0.0074\n","Epoch 23 Loss 0.0082\n","Epoch 24 Loss 0.0076\n","Epoch 25 Loss 0.0073\n","Epoch 26 Loss 0.0066\n","Epoch 27 Loss 0.0061\n","Epoch 28 Loss 0.0064\n","Epoch 29 Loss 0.0065\n","Epoch 30 Loss 0.0063\n","Epoch 31 Loss 0.0062\n","Epoch 32 Loss 0.0065\n","Epoch 33 Loss 0.0061\n","Epoch 34 Loss 0.0060\n","Epoch 35 Loss 0.0065\n","Epoch 36 Loss 0.0055\n","Epoch 37 Loss 0.0054\n","Epoch 38 Loss 0.0055\n","Epoch 39 Loss 0.0054\n","Epoch 40 Loss 0.0052\n","Epoch 41 Loss 0.0065\n","Epoch 42 Loss 0.0053\n","Epoch 43 Loss 0.0052\n","Epoch 44 Loss 0.0048\n","Epoch 45 Loss 0.0046\n","Epoch 46 Loss 0.0070\n","Epoch 47 Loss 0.0057\n","Epoch 48 Loss 0.0050\n","Epoch 49 Loss 0.0046\n","Epoch 50 Loss 0.0045\n","Epoch 51 Loss 0.0047\n","Epoch 52 Loss 0.0054\n","Epoch 53 Loss 0.0053\n","Epoch 54 Loss 0.0067\n","Epoch 55 Loss 0.0054\n","Epoch 56 Loss 0.0051\n","Epoch 57 Loss 0.0045\n","Epoch 58 Loss 0.0044\n","Epoch 59 Loss 0.0044\n","Epoch 60 Loss 0.0043\n","Epoch 61 Loss 0.0045\n","Epoch 62 Loss 0.0066\n","Epoch 63 Loss 0.0055\n","Epoch 64 Loss 0.0048\n","Epoch 65 Loss 0.0046\n","Epoch 66 Loss 0.0044\n","Epoch 67 Loss 0.0044\n","Epoch 68 Loss 0.0043\n","Epoch 69 Loss 0.0044\n","Epoch 70 Loss 0.0045\n","Epoch 71 Loss 0.0047\n","Epoch 72 Loss 0.0056\n","Epoch 73 Loss 0.0059\n","Epoch 74 Loss 0.0052\n","Epoch 75 Loss 0.0047\n","Epoch 76 Loss 0.0045\n","Epoch 77 Loss 0.0044\n","Epoch 78 Loss 0.0044\n","Epoch 79 Loss 0.0045\n","Epoch 80 Loss 0.0046\n","Epoch 81 Loss 0.0044\n","Epoch 82 Loss 0.0044\n","Epoch 83 Loss 0.0043\n","Epoch 84 Loss 0.0045\n","Epoch 85 Loss 0.0053\n","Epoch 86 Loss 0.0053\n","Epoch 87 Loss 0.0075\n","Epoch 88 Loss 0.0053\n","Epoch 89 Loss 0.0050\n","Epoch 90 Loss 0.0044\n","Epoch 91 Loss 0.0044\n","Epoch 92 Loss 0.0043\n","Epoch 93 Loss 0.0042\n","Epoch 94 Loss 0.0041\n","Epoch 95 Loss 0.0043\n","Epoch 96 Loss 0.0045\n","Epoch 97 Loss 0.0043\n","Epoch 98 Loss 0.0046\n","Epoch 99 Loss 0.0051\n","Epoch 100 Loss 0.0052\n","CPU times: user 20min 29s, sys: 1min 41s, total: 22min 11s\n","Wall time: 22min 45s\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"65PD0NfUc9c-"},"source":["Некоторые украденные функции для оценки"]},{"cell_type":"code","metadata":{"id":"2Lndb1Wwc9c-","executionInfo":{"status":"ok","timestamp":1606203078044,"user_tz":-180,"elapsed":1837701,"user":{"displayName":"Дмитрий Фукин","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhOgXORSMlpkMpukDZcLijzceS9AaqgT1tfLKD3jA=s64","userId":"04706038827385634385"}}},"source":["max_length_targ, max_length_inp = target_tensor.shape[1], input_tensor.shape[1]\n","\n","def evaluate(sentence):\n","    attention_plot = np.zeros((max_length_targ, max_length_inp))\n","    sentence = preprocess_sentence(sentence)\n","    inputs = [inp_lang_tokenizer.word_index[i] for i in sentence.split(' ')]\n","    inputs = tf.keras.preprocessing.sequence.pad_sequences([inputs],\n","                                                         maxlen=max_length_inp,\n","                                                         padding='post')\n","    inputs = tf.convert_to_tensor(inputs)\n","    result = ''\n","    hidden = [tf.zeros((1, units))]\n","    enc_out, enc_hidden = encoder(inputs, hidden)\n","    dec_hidden = enc_hidden\n","    dec_input = tf.expand_dims([targ_lang_tokenizer.word_index['<start>']], 0)\n","\n","    for t in range(max_length_targ):\n","        predictions, dec_hidden, attention_weights = decoder(dec_input,\n","                                                             dec_hidden,\n","                                                             enc_out)\n","        attention_weights = tf.reshape(attention_weights, (-1, ))\n","        attention_plot[t] = attention_weights.numpy()\n","\n","        predicted_id = tf.argmax(predictions[0]).numpy()\n","\n","        result += targ_lang_tokenizer.index_word[predicted_id] + ' '\n","\n","        if targ_lang_tokenizer.index_word[predicted_id] == '<end>':\n","            return result, sentence, attention_plot\n","        dec_input = tf.expand_dims([predicted_id], 0)\n","    return result, sentence, attention_plot\n","\n","def plot_attention(attention, sentence, predicted_sentence):\n","    fig = plt.figure(figsize=(10,10))\n","    ax = fig.add_subplot(1, 1, 1)\n","    ax.matshow(attention, cmap='viridis')\n","    fontdict = {'fontsize': 14}\n","    ax.set_xticklabels([''] + sentence, fontdict=fontdict, rotation=90)\n","    ax.set_yticklabels([''] + predicted_sentence, fontdict=fontdict)\n","    #ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n","    #ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n","    plt.show()\n","    \n","def translate(sentence):\n","    result, sentence, attention_plot = evaluate(sentence)\n","    print('Input: %s' % (sentence))\n","    print('Predicted translation: {}'.format(result))\n","    attention_plot = attention_plot[:len(result.split(' ')), :len(sentence.split(' '))]\n","    plot_attention(attention_plot, sentence.split(' '), result.split(' '))"],"execution_count":21,"outputs":[]},{"cell_type":"code","metadata":{"id":"PScQtuOgc9dB","colab":{"base_uri":"https://localhost:8080/","height":620},"executionInfo":{"status":"ok","timestamp":1606203078046,"user_tz":-180,"elapsed":1837696,"user":{"displayName":"Дмитрий Фукин","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhOgXORSMlpkMpukDZcLijzceS9AaqgT1tfLKD3jA=s64","userId":"04706038827385634385"}},"outputId":"5b042c56-43e5-4447-c133-8bfeb923e017"},"source":["%pylab inline\n","\n","translate(u'good morning')"],"execution_count":22,"outputs":[{"output_type":"stream","text":["Populating the interactive namespace from numpy and matplotlib\n","Input: <start> good morning <end>\n","Predicted translation: ! <end> \n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/IPython/core/magics/pylab.py:161: UserWarning: pylab import has clobbered these variables: ['f', 'char']\n","`%matplotlib` prevents importing * from pylab and numpy\n","  \"\\n`%matplotlib` prevents importing * from pylab and numpy\"\n"],"name":"stderr"},{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 720x720 with 1 Axes>"]},"metadata":{"tags":[],"needs_background":"light"}}]},{"cell_type":"code","metadata":{"id":"oBxK_vDHftxl","colab":{"base_uri":"https://localhost:8080/","height":466},"executionInfo":{"status":"ok","timestamp":1606203078388,"user_tz":-180,"elapsed":1838031,"user":{"displayName":"Дмитрий Фукин","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhOgXORSMlpkMpukDZcLijzceS9AaqgT1tfLKD3jA=s64","userId":"04706038827385634385"}},"outputId":"b07c4fb8-87ac-49dd-c0e0-d640edf56881"},"source":["translate(u'how are you')"],"execution_count":23,"outputs":[{"output_type":"stream","text":["Input: <start> how are you <end>\n","Predicted translation: . <end> \n"],"name":"stdout"},{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 720x720 with 1 Axes>"]},"metadata":{"tags":[],"needs_background":"light"}}]},{"cell_type":"code","metadata":{"id":"M5Bnfl6Cfzpr","colab":{"base_uri":"https://localhost:8080/","height":451},"executionInfo":{"status":"ok","timestamp":1606203078675,"user_tz":-180,"elapsed":1838309,"user":{"displayName":"Дмитрий Фукин","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhOgXORSMlpkMpukDZcLijzceS9AaqgT1tfLKD3jA=s64","userId":"04706038827385634385"}},"outputId":"894ff2bd-e5fb-40ef-ea36-edadaae974d1"},"source":["translate(u'you are my best friend')"],"execution_count":24,"outputs":[{"output_type":"stream","text":["Input: <start> you are my best friend <end>\n","Predicted translation: , . <end> \n"],"name":"stdout"},{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 720x720 with 1 Axes>"]},"metadata":{"tags":[],"needs_background":"light"}}]},{"cell_type":"markdown","metadata":{"id":"8D_vmVw3c9dE"},"source":["# Transformer\n","\n","Идея в том, что каждое слово параллельно проходит через слои, изображенные на картинке.\n","Некоторые из них — это стандартные fully-connected layers, некоторые — shortcut connections как в ResNet (там, где на картинке Add).\n","\n","\n","Multi-head attention - это специальный новый слой, который дает возможность каждому входному вектору взаимодействовать с другими словами через attention mechanism, вместо передачи hidden state как в RNN или соседних слов как в CNN.\n","\n","Работа энкодера:\n","\n","\n","Делаются эмбеддинги для всех слов предложения (вектора одинаковой размерности). Для примера пусть это будет предложение I am stupid. В эмбеддинг добавляется еще позиция слова в предложении.\n","\n","Берется вектор первого слова и вектор второго слова (I, am), подаются на однослойную сеть с одним выходом, которая выдает степень их похожести (скалярная величина). Эта скалярная величина умножается на вектор второго слова, получая его некоторую \"ослабленную\" на величину похожести копию.\n","\n","Вместо второго слова подается третье слово и делается тоже самое что в п.2. с той же самой сетью с теми же весами (для векторов I, stupid).\n","\n","Делая тоже самое для всех оставшихся слов предложения получаются их \"ослабленные\" (взвешенные) копии, которые выражают степень их похожести на первое слово. Далее эти все взвешенные вектора складываются друг с другом, получая один результирующий вектор размерности одного эмбединга:\n","output=am * weight(I, am) + stupid * weight(I, stupid)\n","\n","Это механизм \"обычного\" attention.\n","Так как оценка похожести слов всего одним способом (по одному критерию) считается недостаточной, тоже самое (п.2-4) повторяется несколько раз с другими весами. Типа одна один attention может определять похожесть слов по смысловой нагрузке, другой по грамматической, остальные еще как-то и т.п.\n","\n","\n","На выходе п.5. получается несколько векторов, каждый из которых является взвешенной суммой всех остальных слов предложения относительно их похожести на первое слово (I). Конкантенируем этот вректор в один.\n","\n","Дальше ставится еще один слой линейного преобразования, уменьшающий размерность результата п.6. до размерности вектора одного эмбединга. Получается некое представление первого слова предложения, составленное из взвешенных векторов всех остальных слов предложения.\n","\n","Такой же процесс производится для всех других слов в предложении.\n","\n","Так как размерность выхода та же, то можно проделать все тоже самое еще раз (п.2-8), но вместо оригинальных эмбеддингов слов взять то, что получается после прохода через этот Multi-head attention, а нейросети аттеншенов внутри взять с другими весами (веса между слоями не общие). И таких слоев можно сделать много (у гугла 6). Однако между первым и вторым слоем добавляется еще полносвязный слой и residual соединения, чтобы добавить сети выразительности."]},{"cell_type":"code","metadata":{"id":"1ATZClhsc9dF","executionInfo":{"status":"ok","timestamp":1606203078677,"user_tz":-180,"elapsed":1838309,"user":{"displayName":"Дмитрий Фукин","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhOgXORSMlpkMpukDZcLijzceS9AaqgT1tfLKD3jA=s64","userId":"04706038827385634385"}}},"source":["def scaled_dot_product_attention(q, k, v, mask):\n","    \"\"\"Calculate the attention weights.\n","  q, k, v must have matching leading dimensions.\n","  k, v must have matching penultimate dimension, i.e.: seq_len_k = seq_len_v.\n","  The mask has different shapes depending on its type(padding or look ahead) \n","  but it must be broadcastable for addition.\n","  \n","  Args:\n","    q: query shape == (..., seq_len_q, depth)\n","    k: key shape == (..., seq_len_k, depth)\n","    v: value shape == (..., seq_len_v, depth_v)\n","    mask: Float tensor with shape broadcastable \n","          to (..., seq_len_q, seq_len_k). Defaults to None.\n","    \n","  Returns:\n","    output, attention_weights\n","    \"\"\"\n","\n","    matmul_qk = tf.matmul(q, k, transpose_b=True)  # (..., seq_len_q, seq_len_k)\n","  \n","  # scale matmul_qk\n","    dk = tf.cast(tf.shape(k)[-1], tf.float32)\n","    scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)\n","\n","  # add the mask to the scaled tensor.\n","    if mask is not None:\n","        scaled_attention_logits += (mask * -1e9)  \n","\n","  # softmax is normalized on the last axis (seq_len_k) so that the scores\n","  # add up to 1.\n","    attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)  # (..., seq_len_q, seq_len_k)\n","\n","    output = tf.matmul(attention_weights, v)  # (..., seq_len_q, depth_v)\n","    return output, attention_weights"],"execution_count":25,"outputs":[]},{"cell_type":"code","metadata":{"id":"h697jl3Ec9dI","executionInfo":{"status":"ok","timestamp":1606203078678,"user_tz":-180,"elapsed":1838308,"user":{"displayName":"Дмитрий Фукин","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhOgXORSMlpkMpukDZcLijzceS9AaqgT1tfLKD3jA=s64","userId":"04706038827385634385"}}},"source":["class MultiHeadAttention(tf.keras.layers.Layer):\n","    def __init__(self, d_model, num_heads):\n","        super(MultiHeadAttention, self).__init__()\n","        self.num_heads = num_heads\n","        self.d_model = d_model\n","\n","        assert d_model % self.num_heads == 0\n","\n","        self.depth = d_model // self.num_heads\n","\n","        self.wq = tf.keras.layers.Dense(d_model)\n","        self.wk = tf.keras.layers.Dense(d_model)\n","        self.wv = tf.keras.layers.Dense(d_model)\n","\n","        self.dense = tf.keras.layers.Dense(d_model)\n","        \n","    def split_heads(self, x, batch_size):\n","        \"\"\"Split the last dimension into (num_heads, depth).\n","        Transpose the result such that the shape is (batch_size, num_heads, seq_len, depth)\n","        \"\"\"\n","        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n","        return tf.transpose(x, perm=[0, 2, 1, 3])\n","    \n","    def call(self, v, k, q, mask):\n","        batch_size = tf.shape(q)[0]\n","\n","        q = self.wq(q)  # (batch_size, seq_len, d_model)\n","        k = self.wk(k)  # (batch_size, seq_len, d_model)\n","        v = self.wv(v)  # (batch_size, seq_len, d_model)\n","\n","        q = self.split_heads(q, batch_size)  # (batch_size, num_heads, seq_len_q, depth)\n","        k = self.split_heads(k, batch_size)  # (batch_size, num_heads, seq_len_k, depth)\n","        v = self.split_heads(v, batch_size)  # (batch_size, num_heads, seq_len_v, depth)\n","\n","        # scaled_attention.shape == (batch_size, num_heads, seq_len_q, depth)\n","        # attention_weights.shape == (batch_size, num_heads, seq_len_q, seq_len_k)\n","        scaled_attention, attention_weights = scaled_dot_product_attention(\n","            q, k, v, mask)\n","\n","        scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])  # (batch_size, seq_len_q, num_heads, depth)\n","\n","        concat_attention = tf.reshape(scaled_attention, \n","                                      (batch_size, -1, self.d_model))  # (batch_size, seq_len_q, d_model)\n","\n","        output = self.dense(concat_attention)  # (batch_size, seq_len_q, d_model)\n","\n","        return output, attention_weights\n","    \n","def point_wise_feed_forward_network(d_model, dff):\n","    return tf.keras.Sequential([\n","      tf.keras.layers.Dense(dff, activation='relu'),  # (batch_size, seq_len, dff)\n","      tf.keras.layers.Dense(d_model)  # (batch_size, seq_len, d_model)\n","  ])"],"execution_count":26,"outputs":[]},{"cell_type":"code","metadata":{"id":"50C9F499c9dK","executionInfo":{"status":"ok","timestamp":1606203078678,"user_tz":-180,"elapsed":1838306,"user":{"displayName":"Дмитрий Фукин","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhOgXORSMlpkMpukDZcLijzceS9AaqgT1tfLKD3jA=s64","userId":"04706038827385634385"}}},"source":["class EncoderLayer(tf.keras.layers.Layer):\n","    def __init__(self, d_model, num_heads, dff, rate=0.1):\n","        super(EncoderLayer, self).__init__()\n","\n","        self.mha = MultiHeadAttention(d_model, num_heads)\n","        self.ffn = point_wise_feed_forward_network(d_model, dff)\n","\n","        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n","        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n","\n","        self.dropout1 = tf.keras.layers.Dropout(rate)\n","        self.dropout2 = tf.keras.layers.Dropout(rate)\n","    \n","    def call(self, x, training, mask):\n","\n","        attn_output, _ = self.mha(x, x, x, mask)  # (batch_size, input_seq_len, d_model)\n","        attn_output = self.dropout1(attn_output, training=training)\n","        out1 = self.layernorm1(x + attn_output)  # (batch_size, input_seq_len, d_model)\n","\n","        ffn_output = self.ffn(out1)  # (batch_size, input_seq_len, d_model)\n","        ffn_output = self.dropout2(ffn_output, training=training)\n","        out2 = self.layernorm2(out1 + ffn_output)  # (batch_size, input_seq_len, d_model)\n","\n","        return out2\n","    \n","class DecoderLayer(tf.keras.layers.Layer):\n","    def __init__(self, d_model, num_heads, dff, rate=0.1):\n","        super(DecoderLayer, self).__init__()\n","\n","        self.mha1 = MultiHeadAttention(d_model, num_heads)\n","        self.mha2 = MultiHeadAttention(d_model, num_heads)\n","\n","        self.ffn = point_wise_feed_forward_network(d_model, dff)\n","\n","        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n","        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n","        self.layernorm3 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n","\n","        self.dropout1 = tf.keras.layers.Dropout(rate)\n","        self.dropout2 = tf.keras.layers.Dropout(rate)\n","        self.dropout3 = tf.keras.layers.Dropout(rate)\n","\n","    \n","    def call(self, x, enc_output, training, \n","           look_ahead_mask, padding_mask):\n","        # enc_output.shape == (batch_size, input_seq_len, d_model)\n","\n","        attn1, attn_weights_block1 = self.mha1(x, x, x, look_ahead_mask)  # (batch_size, target_seq_len, d_model)\n","        attn1 = self.dropout1(attn1, training=training)\n","        out1 = self.layernorm1(attn1 + x)\n","\n","        attn2, attn_weights_block2 = self.mha2(\n","            enc_output, enc_output, out1, padding_mask)  # (batch_size, target_seq_len, d_model)\n","        attn2 = self.dropout2(attn2, training=training)\n","        out2 = self.layernorm2(attn2 + out1)  # (batch_size, target_seq_len, d_model)\n","\n","        ffn_output = self.ffn(out2)  # (batch_size, target_seq_len, d_model)\n","        ffn_output = self.dropout3(ffn_output, training=training)\n","        out3 = self.layernorm3(ffn_output + out2)  # (batch_size, target_seq_len, d_model)\n","\n","        return out3, attn_weights_block1, attn_weights_block2\n","    \n","    \n","class Encoder(tf.keras.layers.Layer):\n","    def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size,\n","               maximum_position_encoding, rate=0.1):\n","        super(Encoder, self).__init__()\n","\n","        self.d_model = d_model\n","        self.num_layers = num_layers\n","\n","        self.embedding = tf.keras.layers.Embedding(input_vocab_size, d_model)\n","        self.pos_encoding = positional_encoding(maximum_position_encoding, \n","                                                self.d_model)\n","\n","\n","        self.enc_layers = [EncoderLayer(d_model, num_heads, dff, rate) \n","                           for _ in range(num_layers)]\n","\n","        self.dropout = tf.keras.layers.Dropout(rate)\n","        \n","    def call(self, x, training, mask):\n","\n","        seq_len = tf.shape(x)[1]\n","\n","        # adding embedding and position encoding.\n","        x = self.embedding(x)  # (batch_size, input_seq_len, d_model)\n","        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n","        x += self.pos_encoding[:, :seq_len, :]\n","\n","        x = self.dropout(x, training=training)\n","\n","        for i in range(self.num_layers):\n","            x = self.enc_layers[i](x, training, mask)\n","\n","        return x  # (batch_size, input_seq_len, d_model)"],"execution_count":27,"outputs":[]},{"cell_type":"code","metadata":{"id":"3a0JUbjtc9dN","executionInfo":{"status":"ok","timestamp":1606203078679,"user_tz":-180,"elapsed":1838305,"user":{"displayName":"Дмитрий Фукин","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhOgXORSMlpkMpukDZcLijzceS9AaqgT1tfLKD3jA=s64","userId":"04706038827385634385"}}},"source":["class Decoder(tf.keras.layers.Layer):\n","    def __init__(self, num_layers, d_model, num_heads, dff, target_vocab_size,\n","               maximum_position_encoding, rate=0.1):\n","        super(Decoder, self).__init__()\n","\n","        self.d_model = d_model\n","        self.num_layers = num_layers\n","\n","        self.embedding = tf.keras.layers.Embedding(target_vocab_size, d_model)\n","        self.pos_encoding = positional_encoding(maximum_position_encoding, d_model)\n","\n","        self.dec_layers = [DecoderLayer(d_model, num_heads, dff, rate) \n","                           for _ in range(num_layers)]\n","        self.dropout = tf.keras.layers.Dropout(rate)\n","    \n","    def call(self, x, enc_output, training, \n","           look_ahead_mask, padding_mask):\n","\n","        seq_len = tf.shape(x)[1]\n","        attention_weights = {}\n","\n","        x = self.embedding(x)  # (batch_size, target_seq_len, d_model)\n","        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n","        x += self.pos_encoding[:, :seq_len, :]\n","\n","        x = self.dropout(x, training=training)\n","\n","        for i in range(self.num_layers):\n","            x, block1, block2 = self.dec_layers[i](x, enc_output, training,\n","                                                 look_ahead_mask, padding_mask)\n","\n","            attention_weights['decoder_layer{}_block1'.format(i+1)] = block1\n","            attention_weights['decoder_layer{}_block2'.format(i+1)] = block2\n","\n","        # x.shape == (batch_size, target_seq_len, d_model)\n","        return x, attention_weights"],"execution_count":28,"outputs":[]},{"cell_type":"code","metadata":{"id":"T0kDO_DVc9dQ","executionInfo":{"status":"ok","timestamp":1606203078679,"user_tz":-180,"elapsed":1838304,"user":{"displayName":"Дмитрий Фукин","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhOgXORSMlpkMpukDZcLijzceS9AaqgT1tfLKD3jA=s64","userId":"04706038827385634385"}}},"source":["class Transformer(tf.keras.Model):\n","    def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size, \n","               target_vocab_size, pe_input, pe_target, rate=0.1):\n","        super(Transformer, self).__init__()\n","\n","        self.encoder = Encoder(num_layers, d_model, num_heads, dff, \n","                               input_vocab_size, pe_input, rate)\n","\n","        self.decoder = Decoder(num_layers, d_model, num_heads, dff, \n","                               target_vocab_size, pe_target, rate)\n","\n","        self.final_layer = tf.keras.layers.Dense(target_vocab_size)\n","    \n","    def call(self, inp, tar, training, enc_padding_mask, \n","           look_ahead_mask, dec_padding_mask):\n","\n","        enc_output = self.encoder(inp, training, enc_padding_mask)  # (batch_size, inp_seq_len, d_model)\n","\n","        # dec_output.shape == (batch_size, tar_seq_len, d_model)\n","        dec_output, attention_weights = self.decoder(\n","            tar, enc_output, training, look_ahead_mask, dec_padding_mask)\n","\n","        final_output = self.final_layer(dec_output)  # (batch_size, tar_seq_len, target_vocab_size)\n","\n","        return final_output, attention_weights"],"execution_count":29,"outputs":[]},{"cell_type":"code","metadata":{"id":"hNrh-8zjc9dU","executionInfo":{"status":"ok","timestamp":1606203078680,"user_tz":-180,"elapsed":1838303,"user":{"displayName":"Дмитрий Фукин","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhOgXORSMlpkMpukDZcLijzceS9AaqgT1tfLKD3jA=s64","userId":"04706038827385634385"}}},"source":["num_layers = 4\n","d_model = 128\n","dff = 512\n","num_heads = 8\n","\n","input_vocab_size = len(inp_lang_tokenizer.index_word) + 2\n","target_vocab_size = len(targ_lang_tokenizer.index_word) + 2\n","dropout_rate = 0.1"],"execution_count":30,"outputs":[]},{"cell_type":"code","metadata":{"id":"L43Qnij1c9dX","executionInfo":{"status":"ok","timestamp":1606203078680,"user_tz":-180,"elapsed":1838301,"user":{"displayName":"Дмитрий Фукин","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhOgXORSMlpkMpukDZcLijzceS9AaqgT1tfLKD3jA=s64","userId":"04706038827385634385"}}},"source":["class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n","    def __init__(self, d_model, warmup_steps=4000):\n","        super(CustomSchedule, self).__init__()\n","\n","        self.d_model = d_model\n","        self.d_model = tf.cast(self.d_model, tf.float32)\n","\n","        self.warmup_steps = warmup_steps\n","    \n","    def __call__(self, step):\n","        arg1 = tf.math.rsqrt(step)\n","        arg2 = step * (self.warmup_steps ** -1.5)\n","\n","        return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)"],"execution_count":31,"outputs":[]},{"cell_type":"code","metadata":{"id":"_Tx7mWi8c9da","executionInfo":{"status":"ok","timestamp":1606203078681,"user_tz":-180,"elapsed":1838301,"user":{"displayName":"Дмитрий Фукин","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhOgXORSMlpkMpukDZcLijzceS9AaqgT1tfLKD3jA=s64","userId":"04706038827385634385"}}},"source":["learning_rate = CustomSchedule(d_model)\n","\n","optimizer = tf.keras.optimizers.Adam(learning_rate, beta_1=0.9, beta_2=0.98, \n","                                     epsilon=1e-9)\n","loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n","    from_logits=True, reduction='none')\n","\n","def loss_function(real, pred):\n","    mask = tf.math.logical_not(tf.math.equal(real, 0))\n","    loss_ = loss_object(real, pred)\n","\n","    mask = tf.cast(mask, dtype=loss_.dtype)\n","    loss_ *= mask\n","  \n","    return tf.reduce_sum(loss_)/tf.reduce_sum(mask)"],"execution_count":32,"outputs":[]},{"cell_type":"code","metadata":{"id":"4qTbmYWfc9dd","executionInfo":{"status":"ok","timestamp":1606203078681,"user_tz":-180,"elapsed":1838298,"user":{"displayName":"Дмитрий Фукин","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhOgXORSMlpkMpukDZcLijzceS9AaqgT1tfLKD3jA=s64","userId":"04706038827385634385"}}},"source":["def get_angles(pos, i, d_model):\n","    angle_rates = 1 / np.power(10000, (2 * (i//2)) / np.float32(d_model))\n","    return pos * angle_rates\n","\n","def positional_encoding(position, d_model):\n","    angle_rads = get_angles(np.arange(position)[:, np.newaxis],\n","                          np.arange(d_model)[np.newaxis, :],\n","                          d_model)\n","  \n","    angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n","  \n","    angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n","    \n","    pos_encoding = angle_rads[np.newaxis, ...]\n","    \n","    return tf.cast(pos_encoding, dtype=tf.float32)"],"execution_count":33,"outputs":[]},{"cell_type":"code","metadata":{"id":"VTFb65CRc9di","executionInfo":{"status":"ok","timestamp":1606203078682,"user_tz":-180,"elapsed":1838297,"user":{"displayName":"Дмитрий Фукин","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhOgXORSMlpkMpukDZcLijzceS9AaqgT1tfLKD3jA=s64","userId":"04706038827385634385"}}},"source":["def create_masks(inp, tar):\n","    enc_padding_mask = create_padding_mask(inp)\n","  \n","    dec_padding_mask = create_padding_mask(inp)\n","  \n","    look_ahead_mask = create_look_ahead_mask(tf.shape(tar)[1])\n","    dec_target_padding_mask = create_padding_mask(tar)\n","    combined_mask = tf.maximum(dec_target_padding_mask, look_ahead_mask)\n","  \n","    return enc_padding_mask, combined_mask, dec_padding_mask"],"execution_count":34,"outputs":[]},{"cell_type":"code","metadata":{"id":"I47VoYdwc9df","executionInfo":{"status":"ok","timestamp":1606203078949,"user_tz":-180,"elapsed":1838562,"user":{"displayName":"Дмитрий Фукин","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhOgXORSMlpkMpukDZcLijzceS9AaqgT1tfLKD3jA=s64","userId":"04706038827385634385"}}},"source":["train_loss = tf.keras.metrics.Mean(name='train_loss')\n","train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(\n","    name='train_accuracy')\n","\n","transformer = Transformer(num_layers, d_model, num_heads, dff,\n","                          input_vocab_size, target_vocab_size, \n","                          pe_input=input_vocab_size, \n","                          pe_target=target_vocab_size,\n","                          rate=dropout_rate)"],"execution_count":35,"outputs":[]},{"cell_type":"code","metadata":{"id":"OtcuDDQwc9dk","executionInfo":{"status":"ok","timestamp":1606203078952,"user_tz":-180,"elapsed":1838563,"user":{"displayName":"Дмитрий Фукин","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhOgXORSMlpkMpukDZcLijzceS9AaqgT1tfLKD3jA=s64","userId":"04706038827385634385"}}},"source":["train_step_signature = [\n","    tf.TensorSpec(shape=(None, None), dtype=tf.int32),\n","    tf.TensorSpec(shape=(None, None), dtype=tf.int32),\n","]\n","\n","@tf.function(input_signature=train_step_signature)\n","def train_step(inp, tar):\n","    tar_inp = tar[:, :-1]\n","    tar_real = tar[:, 1:]\n","  \n","    enc_padding_mask, combined_mask, dec_padding_mask = create_masks(inp, tar_inp)\n","  \n","    with tf.GradientTape() as tape:\n","        predictions, _ = transformer(inp, tar_inp, \n","                                 True, \n","                                 enc_padding_mask, \n","                                 combined_mask, \n","                                 dec_padding_mask)\n","        loss = loss_function(tar_real, predictions)\n","\n","    gradients = tape.gradient(loss, transformer.trainable_variables)    \n","    optimizer.apply_gradients(zip(gradients, transformer.trainable_variables))\n","  \n","    train_loss(loss)\n","    train_accuracy(tar_real, predictions)"],"execution_count":36,"outputs":[]},{"cell_type":"code","metadata":{"id":"-K_91dy4c9dn","executionInfo":{"status":"ok","timestamp":1606203078954,"user_tz":-180,"elapsed":1838564,"user":{"displayName":"Дмитрий Фукин","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhOgXORSMlpkMpukDZcLijzceS9AaqgT1tfLKD3jA=s64","userId":"04706038827385634385"}}},"source":["def create_padding_mask(seq):\n","    seq = tf.cast(tf.math.equal(seq, 0), tf.float32)\n","  \n","  # add extra dimensions to add the padding\n","  # to the attention logits.\n","    return seq[:, tf.newaxis, tf.newaxis, :]  # (batch_size, 1, 1, seq_len)\n","\n","def create_look_ahead_mask(size):\n","    mask = 1 - tf.linalg.band_part(tf.ones((size, size)), -1, 0)\n","    return mask  # (seq_len, seq_len)"],"execution_count":37,"outputs":[]},{"cell_type":"code","metadata":{"id":"i_iBTwfmgSDE","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1606204288759,"user_tz":-180,"elapsed":3048362,"user":{"displayName":"Дмитрий Фукин","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhOgXORSMlpkMpukDZcLijzceS9AaqgT1tfLKD3jA=s64","userId":"04706038827385634385"}},"outputId":"dbbba345-bccd-4c1f-efc1-1eadacb43cda"},"source":["%%time\n","EPOCHS = 100\n","for epoch in range(EPOCHS):\n","    train_loss.reset_states()\n","    train_accuracy.reset_states()\n","  \n","    for (batch, (inp, tar)) in enumerate(dataset.take(steps_per_epoch)):\n","        train_step(inp, tar)\n","        if batch % 50 == 0:\n","            print ('Epoch {} Batch {} Loss {:.4f} Accuracy {:.4f}'.format(\n","              epoch + 1, batch, train_loss.result(), train_accuracy.result()))\n","    \n","    print ('Epoch {} Loss {:.4f} Accuracy {:.4f}'.format(epoch + 1, \n","                                                train_loss.result(), \n","                                                train_accuracy.result()))"],"execution_count":38,"outputs":[{"output_type":"stream","text":["Epoch 1 Batch 0 Loss 2.1662 Accuracy 0.2539\n","Epoch 1 Batch 50 Loss 1.5191 Accuracy 0.2491\n","Epoch 1 Batch 100 Loss 1.2564 Accuracy 0.2751\n","Epoch 1 Batch 150 Loss 0.9742 Accuracy 0.3349\n","Epoch 1 Batch 200 Loss 0.7780 Accuracy 0.3722\n","Epoch 1 Batch 250 Loss 0.6468 Accuracy 0.3964\n","Epoch 1 Batch 300 Loss 0.5620 Accuracy 0.4124\n","Epoch 1 Loss 0.5461 Accuracy 0.4153\n","Epoch 2 Batch 0 Loss 0.1621 Accuracy 0.4961\n","Epoch 2 Batch 50 Loss 0.1347 Accuracy 0.4930\n","Epoch 2 Batch 100 Loss 0.1225 Accuracy 0.4930\n","Epoch 2 Batch 150 Loss 0.1115 Accuracy 0.4935\n","Epoch 2 Batch 200 Loss 0.1077 Accuracy 0.4938\n","Epoch 2 Batch 250 Loss 0.1052 Accuracy 0.4939\n","Epoch 2 Batch 300 Loss 0.1024 Accuracy 0.4943\n","Epoch 2 Loss 0.1015 Accuracy 0.4944\n","Epoch 3 Batch 0 Loss 0.0371 Accuracy 0.4961\n","Epoch 3 Batch 50 Loss 0.0830 Accuracy 0.4956\n","Epoch 3 Batch 100 Loss 0.0813 Accuracy 0.4960\n","Epoch 3 Batch 150 Loss 0.0804 Accuracy 0.4965\n","Epoch 3 Batch 200 Loss 0.0766 Accuracy 0.4973\n","Epoch 3 Batch 250 Loss 0.0756 Accuracy 0.4974\n","Epoch 3 Batch 300 Loss 0.0748 Accuracy 0.4976\n","Epoch 3 Loss 0.0751 Accuracy 0.4977\n","Epoch 4 Batch 0 Loss 0.0092 Accuracy 0.5000\n","Epoch 4 Batch 50 Loss 0.0624 Accuracy 0.4985\n","Epoch 4 Batch 100 Loss 0.0666 Accuracy 0.4986\n","Epoch 4 Batch 150 Loss 0.0668 Accuracy 0.4988\n","Epoch 4 Batch 200 Loss 0.0668 Accuracy 0.4993\n","Epoch 4 Batch 250 Loss 0.0673 Accuracy 0.4992\n","Epoch 4 Batch 300 Loss 0.0668 Accuracy 0.4991\n","Epoch 4 Loss 0.0672 Accuracy 0.4992\n","Epoch 5 Batch 0 Loss 0.0412 Accuracy 0.5000\n","Epoch 5 Batch 50 Loss 0.0660 Accuracy 0.5012\n","Epoch 5 Batch 100 Loss 0.0616 Accuracy 0.5004\n","Epoch 5 Batch 150 Loss 0.0637 Accuracy 0.5000\n","Epoch 5 Batch 200 Loss 0.0652 Accuracy 0.4998\n","Epoch 5 Batch 250 Loss 0.0651 Accuracy 0.4997\n","Epoch 5 Batch 300 Loss 0.0636 Accuracy 0.4995\n","Epoch 5 Loss 0.0628 Accuracy 0.4995\n","Epoch 6 Batch 0 Loss 0.0387 Accuracy 0.5078\n","Epoch 6 Batch 50 Loss 0.0613 Accuracy 0.5001\n","Epoch 6 Batch 100 Loss 0.0669 Accuracy 0.4998\n","Epoch 6 Batch 150 Loss 0.0682 Accuracy 0.4994\n","Epoch 6 Batch 200 Loss 0.0686 Accuracy 0.4992\n","Epoch 6 Batch 250 Loss 0.0653 Accuracy 0.4992\n","Epoch 6 Batch 300 Loss 0.0659 Accuracy 0.4993\n","Epoch 6 Loss 0.0656 Accuracy 0.4994\n","Epoch 7 Batch 0 Loss 0.0175 Accuracy 0.5039\n","Epoch 7 Batch 50 Loss 0.0700 Accuracy 0.5003\n","Epoch 7 Batch 100 Loss 0.0635 Accuracy 0.5001\n","Epoch 7 Batch 150 Loss 0.0654 Accuracy 0.4994\n","Epoch 7 Batch 200 Loss 0.0640 Accuracy 0.4995\n","Epoch 7 Batch 250 Loss 0.0642 Accuracy 0.4995\n","Epoch 7 Batch 300 Loss 0.0628 Accuracy 0.5000\n","Epoch 7 Loss 0.0625 Accuracy 0.4999\n","Epoch 8 Batch 0 Loss 0.0283 Accuracy 0.5039\n","Epoch 8 Batch 50 Loss 0.0541 Accuracy 0.4992\n","Epoch 8 Batch 100 Loss 0.0716 Accuracy 0.4979\n","Epoch 8 Batch 150 Loss 0.0701 Accuracy 0.4991\n","Epoch 8 Batch 200 Loss 0.0664 Accuracy 0.4989\n","Epoch 8 Batch 250 Loss 0.0681 Accuracy 0.4988\n","Epoch 8 Batch 300 Loss 0.0658 Accuracy 0.4991\n","Epoch 8 Loss 0.0655 Accuracy 0.4991\n","Epoch 9 Batch 0 Loss 0.0893 Accuracy 0.5000\n","Epoch 9 Batch 50 Loss 0.0721 Accuracy 0.4988\n","Epoch 9 Batch 100 Loss 0.0637 Accuracy 0.4993\n","Epoch 9 Batch 150 Loss 0.0601 Accuracy 0.4994\n","Epoch 9 Batch 200 Loss 0.0603 Accuracy 0.5000\n","Epoch 9 Batch 250 Loss 0.0602 Accuracy 0.5001\n","Epoch 9 Batch 300 Loss 0.0609 Accuracy 0.5000\n","Epoch 9 Loss 0.0606 Accuracy 0.5001\n","Epoch 10 Batch 0 Loss 0.0551 Accuracy 0.5000\n","Epoch 10 Batch 50 Loss 0.0583 Accuracy 0.5002\n","Epoch 10 Batch 100 Loss 0.0552 Accuracy 0.5003\n","Epoch 10 Batch 150 Loss 0.0570 Accuracy 0.4999\n","Epoch 10 Batch 200 Loss 0.0596 Accuracy 0.5001\n","Epoch 10 Batch 250 Loss 0.0650 Accuracy 0.4991\n","Epoch 10 Batch 300 Loss 0.0650 Accuracy 0.4989\n","Epoch 10 Loss 0.0649 Accuracy 0.4990\n","Epoch 11 Batch 0 Loss 0.0647 Accuracy 0.5156\n","Epoch 11 Batch 50 Loss 0.0636 Accuracy 0.4989\n","Epoch 11 Batch 100 Loss 0.0589 Accuracy 0.4992\n","Epoch 11 Batch 150 Loss 0.0587 Accuracy 0.4995\n","Epoch 11 Batch 200 Loss 0.0612 Accuracy 0.4994\n","Epoch 11 Batch 250 Loss 0.0620 Accuracy 0.4997\n","Epoch 11 Batch 300 Loss 0.0633 Accuracy 0.4994\n","Epoch 11 Loss 0.0634 Accuracy 0.4994\n","Epoch 12 Batch 0 Loss 0.0119 Accuracy 0.5117\n","Epoch 12 Batch 50 Loss 0.0570 Accuracy 0.5009\n","Epoch 12 Batch 100 Loss 0.0837 Accuracy 0.4993\n","Epoch 12 Batch 150 Loss 0.0732 Accuracy 0.4996\n","Epoch 12 Batch 200 Loss 0.0750 Accuracy 0.4998\n","Epoch 12 Batch 250 Loss 0.0727 Accuracy 0.4996\n","Epoch 12 Batch 300 Loss 0.0720 Accuracy 0.4996\n","Epoch 12 Loss 0.0714 Accuracy 0.4996\n","Epoch 13 Batch 0 Loss 0.0360 Accuracy 0.5039\n","Epoch 13 Batch 50 Loss 0.0627 Accuracy 0.4995\n","Epoch 13 Batch 100 Loss 0.0589 Accuracy 0.4999\n","Epoch 13 Batch 150 Loss 0.0557 Accuracy 0.5004\n","Epoch 13 Batch 200 Loss 0.0626 Accuracy 0.4993\n","Epoch 13 Batch 250 Loss 0.0632 Accuracy 0.4995\n","Epoch 13 Batch 300 Loss 0.0626 Accuracy 0.4995\n","Epoch 13 Loss 0.0625 Accuracy 0.4996\n","Epoch 14 Batch 0 Loss 0.0095 Accuracy 0.5000\n","Epoch 14 Batch 50 Loss 0.0512 Accuracy 0.5000\n","Epoch 14 Batch 100 Loss 0.0571 Accuracy 0.5000\n","Epoch 14 Batch 150 Loss 0.0608 Accuracy 0.4991\n","Epoch 14 Batch 200 Loss 0.0650 Accuracy 0.4981\n","Epoch 14 Batch 250 Loss 0.0668 Accuracy 0.4974\n","Epoch 14 Batch 300 Loss 0.0656 Accuracy 0.4971\n","Epoch 14 Loss 0.0657 Accuracy 0.4970\n","Epoch 15 Batch 0 Loss 0.0521 Accuracy 0.4961\n","Epoch 15 Batch 50 Loss 0.0676 Accuracy 0.4960\n","Epoch 15 Batch 100 Loss 0.0719 Accuracy 0.4957\n","Epoch 15 Batch 150 Loss 0.0737 Accuracy 0.4958\n","Epoch 15 Batch 200 Loss 0.0748 Accuracy 0.4955\n","Epoch 15 Batch 250 Loss 0.0728 Accuracy 0.4954\n","Epoch 15 Batch 300 Loss 0.0720 Accuracy 0.4955\n","Epoch 15 Loss 0.0710 Accuracy 0.4955\n","Epoch 16 Batch 0 Loss 0.0396 Accuracy 0.5000\n","Epoch 16 Batch 50 Loss 0.0666 Accuracy 0.4962\n","Epoch 16 Batch 100 Loss 0.0700 Accuracy 0.4966\n","Epoch 16 Batch 150 Loss 0.0704 Accuracy 0.4962\n","Epoch 16 Batch 200 Loss 0.0691 Accuracy 0.4962\n","Epoch 16 Batch 250 Loss 0.0672 Accuracy 0.4963\n","Epoch 16 Batch 300 Loss 0.0654 Accuracy 0.4963\n","Epoch 16 Loss 0.0650 Accuracy 0.4963\n","Epoch 17 Batch 0 Loss 0.0403 Accuracy 0.5000\n","Epoch 17 Batch 50 Loss 0.0812 Accuracy 0.4953\n","Epoch 17 Batch 100 Loss 0.0883 Accuracy 0.4943\n","Epoch 17 Batch 150 Loss 0.0874 Accuracy 0.4934\n","Epoch 17 Batch 200 Loss 0.0856 Accuracy 0.4933\n","Epoch 17 Batch 250 Loss 0.0872 Accuracy 0.4934\n","Epoch 17 Batch 300 Loss 0.0927 Accuracy 0.4931\n","Epoch 17 Loss 0.0926 Accuracy 0.4932\n","Epoch 18 Batch 0 Loss 0.0800 Accuracy 0.4961\n","Epoch 18 Batch 50 Loss 0.0769 Accuracy 0.4954\n","Epoch 18 Batch 100 Loss 0.0615 Accuracy 0.4987\n","Epoch 18 Batch 150 Loss 0.0585 Accuracy 0.4999\n","Epoch 18 Batch 200 Loss 0.0586 Accuracy 0.5000\n","Epoch 18 Batch 250 Loss 0.0585 Accuracy 0.5001\n","Epoch 18 Batch 300 Loss 0.0590 Accuracy 0.5001\n","Epoch 18 Loss 0.0591 Accuracy 0.5000\n","Epoch 19 Batch 0 Loss 0.1323 Accuracy 0.4961\n","Epoch 19 Batch 50 Loss 0.0591 Accuracy 0.4999\n","Epoch 19 Batch 100 Loss 0.0535 Accuracy 0.5015\n","Epoch 19 Batch 150 Loss 0.0540 Accuracy 0.5008\n","Epoch 19 Batch 200 Loss 0.0595 Accuracy 0.5003\n","Epoch 19 Batch 250 Loss 0.0598 Accuracy 0.4994\n","Epoch 19 Batch 300 Loss 0.0592 Accuracy 0.4995\n","Epoch 19 Loss 0.0589 Accuracy 0.4996\n","Epoch 20 Batch 0 Loss 0.0135 Accuracy 0.5078\n","Epoch 20 Batch 50 Loss 0.0446 Accuracy 0.5019\n","Epoch 20 Batch 100 Loss 0.0449 Accuracy 0.5021\n","Epoch 20 Batch 150 Loss 0.0445 Accuracy 0.5019\n","Epoch 20 Batch 200 Loss 0.0503 Accuracy 0.5015\n","Epoch 20 Batch 250 Loss 0.0511 Accuracy 0.5011\n","Epoch 20 Batch 300 Loss 0.0517 Accuracy 0.5011\n","Epoch 20 Loss 0.0512 Accuracy 0.5012\n","Epoch 21 Batch 0 Loss 0.0350 Accuracy 0.4961\n","Epoch 21 Batch 50 Loss 0.0540 Accuracy 0.5008\n","Epoch 21 Batch 100 Loss 0.0518 Accuracy 0.5008\n","Epoch 21 Batch 150 Loss 0.0512 Accuracy 0.5009\n","Epoch 21 Batch 200 Loss 0.0503 Accuracy 0.5009\n","Epoch 21 Batch 250 Loss 0.0507 Accuracy 0.5009\n","Epoch 21 Batch 300 Loss 0.0497 Accuracy 0.5013\n","Epoch 21 Loss 0.0496 Accuracy 0.5012\n","Epoch 22 Batch 0 Loss 0.0195 Accuracy 0.5039\n","Epoch 22 Batch 50 Loss 0.0466 Accuracy 0.5028\n","Epoch 22 Batch 100 Loss 0.0465 Accuracy 0.5020\n","Epoch 22 Batch 150 Loss 0.0464 Accuracy 0.5021\n","Epoch 22 Batch 200 Loss 0.0471 Accuracy 0.5018\n","Epoch 22 Batch 250 Loss 0.0458 Accuracy 0.5019\n","Epoch 22 Batch 300 Loss 0.0478 Accuracy 0.5017\n","Epoch 22 Loss 0.0475 Accuracy 0.5017\n","Epoch 23 Batch 0 Loss 0.0116 Accuracy 0.5078\n","Epoch 23 Batch 50 Loss 0.0483 Accuracy 0.5019\n","Epoch 23 Batch 100 Loss 0.0443 Accuracy 0.5021\n","Epoch 23 Batch 150 Loss 0.0446 Accuracy 0.5019\n","Epoch 23 Batch 200 Loss 0.0501 Accuracy 0.5010\n","Epoch 23 Batch 250 Loss 0.0493 Accuracy 0.5013\n","Epoch 23 Batch 300 Loss 0.0482 Accuracy 0.5014\n","Epoch 23 Loss 0.0482 Accuracy 0.5014\n","Epoch 24 Batch 0 Loss 0.0434 Accuracy 0.4961\n","Epoch 24 Batch 50 Loss 0.0342 Accuracy 0.5026\n","Epoch 24 Batch 100 Loss 0.0381 Accuracy 0.5029\n","Epoch 24 Batch 150 Loss 0.0414 Accuracy 0.5027\n","Epoch 24 Batch 200 Loss 0.0436 Accuracy 0.5022\n","Epoch 24 Batch 250 Loss 0.0442 Accuracy 0.5020\n","Epoch 24 Batch 300 Loss 0.0447 Accuracy 0.5019\n","Epoch 24 Loss 0.0452 Accuracy 0.5019\n","Epoch 25 Batch 0 Loss 0.0716 Accuracy 0.4883\n","Epoch 25 Batch 50 Loss 0.0501 Accuracy 0.5021\n","Epoch 25 Batch 100 Loss 0.0828 Accuracy 0.4998\n","Epoch 25 Batch 150 Loss 0.1141 Accuracy 0.4901\n","Epoch 25 Batch 200 Loss 0.1208 Accuracy 0.4847\n","Epoch 25 Batch 250 Loss 0.1158 Accuracy 0.4848\n","Epoch 25 Batch 300 Loss 0.1053 Accuracy 0.4876\n","Epoch 25 Loss 0.1031 Accuracy 0.4882\n","Epoch 26 Batch 0 Loss 0.0301 Accuracy 0.4961\n","Epoch 26 Batch 50 Loss 0.0408 Accuracy 0.5023\n","Epoch 26 Batch 100 Loss 0.0522 Accuracy 0.5015\n","Epoch 26 Batch 150 Loss 0.0500 Accuracy 0.5018\n","Epoch 26 Batch 200 Loss 0.0503 Accuracy 0.5021\n","Epoch 26 Batch 250 Loss 0.0492 Accuracy 0.5020\n","Epoch 26 Batch 300 Loss 0.0482 Accuracy 0.5019\n","Epoch 26 Loss 0.0479 Accuracy 0.5018\n","Epoch 27 Batch 0 Loss 0.0251 Accuracy 0.5000\n","Epoch 27 Batch 50 Loss 0.0329 Accuracy 0.5013\n","Epoch 27 Batch 100 Loss 0.0397 Accuracy 0.5016\n","Epoch 27 Batch 150 Loss 0.0418 Accuracy 0.5021\n","Epoch 27 Batch 200 Loss 0.0399 Accuracy 0.5022\n","Epoch 27 Batch 250 Loss 0.0422 Accuracy 0.5022\n","Epoch 27 Batch 300 Loss 0.0442 Accuracy 0.5020\n","Epoch 27 Loss 0.0442 Accuracy 0.5020\n","Epoch 28 Batch 0 Loss 0.0642 Accuracy 0.5117\n","Epoch 28 Batch 50 Loss 0.0393 Accuracy 0.5011\n","Epoch 28 Batch 100 Loss 0.0403 Accuracy 0.5014\n","Epoch 28 Batch 150 Loss 0.0439 Accuracy 0.5018\n","Epoch 28 Batch 200 Loss 0.0443 Accuracy 0.5014\n","Epoch 28 Batch 250 Loss 0.0438 Accuracy 0.5016\n","Epoch 28 Batch 300 Loss 0.0428 Accuracy 0.5017\n","Epoch 28 Loss 0.0432 Accuracy 0.5017\n","Epoch 29 Batch 0 Loss 0.0461 Accuracy 0.5078\n","Epoch 29 Batch 50 Loss 0.0383 Accuracy 0.5016\n","Epoch 29 Batch 100 Loss 0.0436 Accuracy 0.5020\n","Epoch 29 Batch 150 Loss 0.0430 Accuracy 0.5019\n","Epoch 29 Batch 200 Loss 0.0417 Accuracy 0.5020\n","Epoch 29 Batch 250 Loss 0.0424 Accuracy 0.5020\n","Epoch 29 Batch 300 Loss 0.0408 Accuracy 0.5022\n","Epoch 29 Loss 0.0409 Accuracy 0.5023\n","Epoch 30 Batch 0 Loss 0.0677 Accuracy 0.5078\n","Epoch 30 Batch 50 Loss 0.0384 Accuracy 0.5018\n","Epoch 30 Batch 100 Loss 0.0449 Accuracy 0.5017\n","Epoch 30 Batch 150 Loss 0.0434 Accuracy 0.5017\n","Epoch 30 Batch 200 Loss 0.0420 Accuracy 0.5019\n","Epoch 30 Batch 250 Loss 0.0430 Accuracy 0.5021\n","Epoch 30 Batch 300 Loss 0.0427 Accuracy 0.5021\n","Epoch 30 Loss 0.0423 Accuracy 0.5022\n","Epoch 31 Batch 0 Loss 0.0264 Accuracy 0.5117\n","Epoch 31 Batch 50 Loss 0.0399 Accuracy 0.5026\n","Epoch 31 Batch 100 Loss 0.0366 Accuracy 0.5029\n","Epoch 31 Batch 150 Loss 0.0389 Accuracy 0.5028\n","Epoch 31 Batch 200 Loss 0.0397 Accuracy 0.5025\n","Epoch 31 Batch 250 Loss 0.0412 Accuracy 0.5022\n","Epoch 31 Batch 300 Loss 0.0413 Accuracy 0.5021\n","Epoch 31 Loss 0.0414 Accuracy 0.5021\n","Epoch 32 Batch 0 Loss 0.1156 Accuracy 0.4883\n","Epoch 32 Batch 50 Loss 0.0413 Accuracy 0.5028\n","Epoch 32 Batch 100 Loss 0.0379 Accuracy 0.5034\n","Epoch 32 Batch 150 Loss 0.0357 Accuracy 0.5028\n","Epoch 32 Batch 200 Loss 0.0374 Accuracy 0.5027\n","Epoch 32 Batch 250 Loss 0.0382 Accuracy 0.5025\n","Epoch 32 Batch 300 Loss 0.0390 Accuracy 0.5024\n","Epoch 32 Loss 0.0387 Accuracy 0.5023\n","Epoch 33 Batch 0 Loss 0.0864 Accuracy 0.4883\n","Epoch 33 Batch 50 Loss 0.0379 Accuracy 0.5012\n","Epoch 33 Batch 100 Loss 0.0349 Accuracy 0.5021\n","Epoch 33 Batch 150 Loss 0.0370 Accuracy 0.5027\n","Epoch 33 Batch 200 Loss 0.0369 Accuracy 0.5024\n","Epoch 33 Batch 250 Loss 0.0377 Accuracy 0.5023\n","Epoch 33 Batch 300 Loss 0.0428 Accuracy 0.5020\n","Epoch 33 Loss 0.0488 Accuracy 0.5012\n","Epoch 34 Batch 0 Loss 0.2301 Accuracy 0.4648\n","Epoch 34 Batch 50 Loss 0.0853 Accuracy 0.4949\n","Epoch 34 Batch 100 Loss 0.0642 Accuracy 0.4982\n","Epoch 34 Batch 150 Loss 0.0550 Accuracy 0.5001\n","Epoch 34 Batch 200 Loss 0.0489 Accuracy 0.5007\n","Epoch 34 Batch 250 Loss 0.0447 Accuracy 0.5012\n","Epoch 34 Batch 300 Loss 0.0452 Accuracy 0.5012\n","Epoch 34 Loss 0.0452 Accuracy 0.5012\n","Epoch 35 Batch 0 Loss 0.0223 Accuracy 0.4961\n","Epoch 35 Batch 50 Loss 0.0371 Accuracy 0.5037\n","Epoch 35 Batch 100 Loss 0.0351 Accuracy 0.5037\n","Epoch 35 Batch 150 Loss 0.0349 Accuracy 0.5034\n","Epoch 35 Batch 200 Loss 0.0902 Accuracy 0.4981\n","Epoch 35 Batch 250 Loss 0.0901 Accuracy 0.4973\n","Epoch 35 Batch 300 Loss 0.0850 Accuracy 0.4969\n","Epoch 35 Loss 0.0842 Accuracy 0.4968\n","Epoch 36 Batch 0 Loss 0.0298 Accuracy 0.4961\n","Epoch 36 Batch 50 Loss 0.0662 Accuracy 0.4953\n","Epoch 36 Batch 100 Loss 0.0651 Accuracy 0.4954\n","Epoch 36 Batch 150 Loss 0.0653 Accuracy 0.4954\n","Epoch 36 Batch 200 Loss 0.0682 Accuracy 0.4952\n","Epoch 36 Batch 250 Loss 0.0715 Accuracy 0.4943\n","Epoch 36 Batch 300 Loss 0.0726 Accuracy 0.4943\n","Epoch 36 Loss 0.0732 Accuracy 0.4942\n","Epoch 37 Batch 0 Loss 0.1130 Accuracy 0.4883\n","Epoch 37 Batch 50 Loss 0.1211 Accuracy 0.4821\n","Epoch 37 Batch 100 Loss 0.1060 Accuracy 0.4866\n","Epoch 37 Batch 150 Loss 0.1150 Accuracy 0.4838\n","Epoch 37 Batch 200 Loss 0.1088 Accuracy 0.4855\n","Epoch 37 Batch 250 Loss 0.1042 Accuracy 0.4868\n","Epoch 37 Batch 300 Loss 0.1022 Accuracy 0.4881\n","Epoch 37 Loss 0.1006 Accuracy 0.4883\n","Epoch 38 Batch 0 Loss 0.0414 Accuracy 0.4961\n","Epoch 38 Batch 50 Loss 0.0717 Accuracy 0.4988\n","Epoch 38 Batch 100 Loss 0.0636 Accuracy 0.4998\n","Epoch 38 Batch 150 Loss 0.0595 Accuracy 0.5008\n","Epoch 38 Batch 200 Loss 0.0575 Accuracy 0.5006\n","Epoch 38 Batch 250 Loss 0.0546 Accuracy 0.5010\n","Epoch 38 Batch 300 Loss 0.0530 Accuracy 0.5011\n","Epoch 38 Loss 0.0521 Accuracy 0.5011\n","Epoch 39 Batch 0 Loss 0.0122 Accuracy 0.5000\n","Epoch 39 Batch 50 Loss 0.0456 Accuracy 0.5011\n","Epoch 39 Batch 100 Loss 0.0439 Accuracy 0.5018\n","Epoch 39 Batch 150 Loss 0.0442 Accuracy 0.5023\n","Epoch 39 Batch 200 Loss 0.0436 Accuracy 0.5024\n","Epoch 39 Batch 250 Loss 0.0418 Accuracy 0.5024\n","Epoch 39 Batch 300 Loss 0.0401 Accuracy 0.5026\n","Epoch 39 Loss 0.0401 Accuracy 0.5027\n","Epoch 40 Batch 0 Loss 0.0461 Accuracy 0.5039\n","Epoch 40 Batch 50 Loss 0.0310 Accuracy 0.5044\n","Epoch 40 Batch 100 Loss 0.0340 Accuracy 0.5036\n","Epoch 40 Batch 150 Loss 0.0334 Accuracy 0.5032\n","Epoch 40 Batch 200 Loss 0.0375 Accuracy 0.5030\n","Epoch 40 Batch 250 Loss 0.0386 Accuracy 0.5031\n","Epoch 40 Batch 300 Loss 0.0375 Accuracy 0.5029\n","Epoch 40 Loss 0.0380 Accuracy 0.5028\n","Epoch 41 Batch 0 Loss 0.0084 Accuracy 0.5000\n","Epoch 41 Batch 50 Loss 0.0406 Accuracy 0.5046\n","Epoch 41 Batch 100 Loss 0.0363 Accuracy 0.5036\n","Epoch 41 Batch 150 Loss 0.0379 Accuracy 0.5030\n","Epoch 41 Batch 200 Loss 0.0397 Accuracy 0.5023\n","Epoch 41 Batch 250 Loss 0.0387 Accuracy 0.5024\n","Epoch 41 Batch 300 Loss 0.0392 Accuracy 0.5026\n","Epoch 41 Loss 0.0396 Accuracy 0.5025\n","Epoch 42 Batch 0 Loss 0.0357 Accuracy 0.5078\n","Epoch 42 Batch 50 Loss 0.0329 Accuracy 0.5030\n","Epoch 42 Batch 100 Loss 0.0383 Accuracy 0.5031\n","Epoch 42 Batch 150 Loss 0.0376 Accuracy 0.5028\n","Epoch 42 Batch 200 Loss 0.0380 Accuracy 0.5028\n","Epoch 42 Batch 250 Loss 0.0375 Accuracy 0.5024\n","Epoch 42 Batch 300 Loss 0.0375 Accuracy 0.5026\n","Epoch 42 Loss 0.0378 Accuracy 0.5026\n","Epoch 43 Batch 0 Loss 0.0290 Accuracy 0.4922\n","Epoch 43 Batch 50 Loss 0.0449 Accuracy 0.5022\n","Epoch 43 Batch 100 Loss 0.0365 Accuracy 0.5027\n","Epoch 43 Batch 150 Loss 0.0375 Accuracy 0.5021\n","Epoch 43 Batch 200 Loss 0.0400 Accuracy 0.5019\n","Epoch 43 Batch 250 Loss 0.0380 Accuracy 0.5025\n","Epoch 43 Batch 300 Loss 0.0365 Accuracy 0.5027\n","Epoch 43 Loss 0.0365 Accuracy 0.5027\n","Epoch 44 Batch 0 Loss 0.0028 Accuracy 0.5117\n","Epoch 44 Batch 50 Loss 0.0264 Accuracy 0.5034\n","Epoch 44 Batch 100 Loss 0.0333 Accuracy 0.5031\n","Epoch 44 Batch 150 Loss 0.0352 Accuracy 0.5030\n","Epoch 44 Batch 200 Loss 0.0366 Accuracy 0.5025\n","Epoch 44 Batch 250 Loss 0.0361 Accuracy 0.5027\n","Epoch 44 Batch 300 Loss 0.0351 Accuracy 0.5029\n","Epoch 44 Loss 0.0354 Accuracy 0.5029\n","Epoch 45 Batch 0 Loss 0.0238 Accuracy 0.4961\n","Epoch 45 Batch 50 Loss 0.0441 Accuracy 0.5027\n","Epoch 45 Batch 100 Loss 0.0398 Accuracy 0.5028\n","Epoch 45 Batch 150 Loss 0.0356 Accuracy 0.5031\n","Epoch 45 Batch 200 Loss 0.0356 Accuracy 0.5031\n","Epoch 45 Batch 250 Loss 0.0354 Accuracy 0.5029\n","Epoch 45 Batch 300 Loss 0.0350 Accuracy 0.5029\n","Epoch 45 Loss 0.0350 Accuracy 0.5028\n","Epoch 46 Batch 0 Loss 0.0204 Accuracy 0.5039\n","Epoch 46 Batch 50 Loss 0.0315 Accuracy 0.5026\n","Epoch 46 Batch 100 Loss 0.0345 Accuracy 0.5029\n","Epoch 46 Batch 150 Loss 0.0354 Accuracy 0.5027\n","Epoch 46 Batch 200 Loss 0.0366 Accuracy 0.5022\n","Epoch 46 Batch 250 Loss 0.0358 Accuracy 0.5023\n","Epoch 46 Batch 300 Loss 0.0356 Accuracy 0.5027\n","Epoch 46 Loss 0.0361 Accuracy 0.5027\n","Epoch 47 Batch 0 Loss 0.0181 Accuracy 0.5039\n","Epoch 47 Batch 50 Loss 0.0353 Accuracy 0.5031\n","Epoch 47 Batch 100 Loss 0.0374 Accuracy 0.5029\n","Epoch 47 Batch 150 Loss 0.0381 Accuracy 0.5029\n","Epoch 47 Batch 200 Loss 0.0356 Accuracy 0.5027\n","Epoch 47 Batch 250 Loss 0.0357 Accuracy 0.5028\n","Epoch 47 Batch 300 Loss 0.0367 Accuracy 0.5027\n","Epoch 47 Loss 0.0370 Accuracy 0.5027\n","Epoch 48 Batch 0 Loss 0.0506 Accuracy 0.4922\n","Epoch 48 Batch 50 Loss 0.0303 Accuracy 0.5022\n","Epoch 48 Batch 100 Loss 0.0333 Accuracy 0.5025\n","Epoch 48 Batch 150 Loss 0.0355 Accuracy 0.5027\n","Epoch 48 Batch 200 Loss 0.0345 Accuracy 0.5028\n","Epoch 48 Batch 250 Loss 0.0348 Accuracy 0.5031\n","Epoch 48 Batch 300 Loss 0.0351 Accuracy 0.5031\n","Epoch 48 Loss 0.0348 Accuracy 0.5031\n","Epoch 49 Batch 0 Loss 0.0242 Accuracy 0.5000\n","Epoch 49 Batch 50 Loss 0.0278 Accuracy 0.5031\n","Epoch 49 Batch 100 Loss 0.0284 Accuracy 0.5030\n","Epoch 49 Batch 150 Loss 0.0347 Accuracy 0.5025\n","Epoch 49 Batch 200 Loss 0.0331 Accuracy 0.5024\n","Epoch 49 Batch 250 Loss 0.0349 Accuracy 0.5025\n","Epoch 49 Batch 300 Loss 0.0339 Accuracy 0.5025\n","Epoch 49 Loss 0.0348 Accuracy 0.5026\n","Epoch 50 Batch 0 Loss 0.1368 Accuracy 0.5000\n","Epoch 50 Batch 50 Loss 0.0361 Accuracy 0.5029\n","Epoch 50 Batch 100 Loss 0.0365 Accuracy 0.5029\n","Epoch 50 Batch 150 Loss 0.0356 Accuracy 0.5028\n","Epoch 50 Batch 200 Loss 0.0347 Accuracy 0.5028\n","Epoch 50 Batch 250 Loss 0.0338 Accuracy 0.5030\n","Epoch 50 Batch 300 Loss 0.0344 Accuracy 0.5030\n","Epoch 50 Loss 0.0339 Accuracy 0.5030\n","Epoch 51 Batch 0 Loss 0.0435 Accuracy 0.5039\n","Epoch 51 Batch 50 Loss 0.0339 Accuracy 0.5029\n","Epoch 51 Batch 100 Loss 0.0363 Accuracy 0.5029\n","Epoch 51 Batch 150 Loss 0.0339 Accuracy 0.5031\n","Epoch 51 Batch 200 Loss 0.0336 Accuracy 0.5033\n","Epoch 51 Batch 250 Loss 0.0334 Accuracy 0.5030\n","Epoch 51 Batch 300 Loss 0.0326 Accuracy 0.5033\n","Epoch 51 Loss 0.0324 Accuracy 0.5031\n","Epoch 52 Batch 0 Loss 0.0394 Accuracy 0.5078\n","Epoch 52 Batch 50 Loss 0.0232 Accuracy 0.5043\n","Epoch 52 Batch 100 Loss 0.0297 Accuracy 0.5035\n","Epoch 52 Batch 150 Loss 0.0288 Accuracy 0.5033\n","Epoch 52 Batch 200 Loss 0.0290 Accuracy 0.5032\n","Epoch 52 Batch 250 Loss 0.0305 Accuracy 0.5033\n","Epoch 52 Batch 300 Loss 0.0320 Accuracy 0.5032\n","Epoch 52 Loss 0.0315 Accuracy 0.5032\n","Epoch 53 Batch 0 Loss 0.0285 Accuracy 0.5039\n","Epoch 53 Batch 50 Loss 0.0229 Accuracy 0.5057\n","Epoch 53 Batch 100 Loss 0.0274 Accuracy 0.5040\n","Epoch 53 Batch 150 Loss 0.0298 Accuracy 0.5036\n","Epoch 53 Batch 200 Loss 0.0304 Accuracy 0.5033\n","Epoch 53 Batch 250 Loss 0.0309 Accuracy 0.5031\n","Epoch 53 Batch 300 Loss 0.0326 Accuracy 0.5029\n","Epoch 53 Loss 0.0328 Accuracy 0.5030\n","Epoch 54 Batch 0 Loss 0.0335 Accuracy 0.5000\n","Epoch 54 Batch 50 Loss 0.0327 Accuracy 0.5031\n","Epoch 54 Batch 100 Loss 0.0345 Accuracy 0.5025\n","Epoch 54 Batch 150 Loss 0.0319 Accuracy 0.5026\n","Epoch 54 Batch 200 Loss 0.0332 Accuracy 0.5026\n","Epoch 54 Batch 250 Loss 0.0343 Accuracy 0.5027\n","Epoch 54 Batch 300 Loss 0.0344 Accuracy 0.5028\n","Epoch 54 Loss 0.0347 Accuracy 0.5028\n","Epoch 55 Batch 0 Loss 0.0426 Accuracy 0.4961\n","Epoch 55 Batch 50 Loss 0.0350 Accuracy 0.5021\n","Epoch 55 Batch 100 Loss 0.0352 Accuracy 0.5028\n","Epoch 55 Batch 150 Loss 0.0349 Accuracy 0.5027\n","Epoch 55 Batch 200 Loss 0.0339 Accuracy 0.5025\n","Epoch 55 Batch 250 Loss 0.0335 Accuracy 0.5026\n","Epoch 55 Batch 300 Loss 0.0339 Accuracy 0.5026\n","Epoch 55 Loss 0.0338 Accuracy 0.5026\n","Epoch 56 Batch 0 Loss 0.0028 Accuracy 0.5117\n","Epoch 56 Batch 50 Loss 0.0350 Accuracy 0.5041\n","Epoch 56 Batch 100 Loss 0.0328 Accuracy 0.5044\n","Epoch 56 Batch 150 Loss 0.0330 Accuracy 0.5036\n","Epoch 56 Batch 200 Loss 0.0341 Accuracy 0.5032\n","Epoch 56 Batch 250 Loss 0.0336 Accuracy 0.5030\n","Epoch 56 Batch 300 Loss 0.0325 Accuracy 0.5032\n","Epoch 56 Loss 0.0328 Accuracy 0.5031\n","Epoch 57 Batch 0 Loss 0.0357 Accuracy 0.5156\n","Epoch 57 Batch 50 Loss 0.0293 Accuracy 0.5035\n","Epoch 57 Batch 100 Loss 0.0323 Accuracy 0.5038\n","Epoch 57 Batch 150 Loss 0.0319 Accuracy 0.5034\n","Epoch 57 Batch 200 Loss 0.0320 Accuracy 0.5036\n","Epoch 57 Batch 250 Loss 0.0328 Accuracy 0.5032\n","Epoch 57 Batch 300 Loss 0.0334 Accuracy 0.5030\n","Epoch 57 Loss 0.0333 Accuracy 0.5030\n","Epoch 58 Batch 0 Loss 0.0236 Accuracy 0.5039\n","Epoch 58 Batch 50 Loss 0.0287 Accuracy 0.5021\n","Epoch 58 Batch 100 Loss 0.0292 Accuracy 0.5024\n","Epoch 58 Batch 150 Loss 0.0294 Accuracy 0.5030\n","Epoch 58 Batch 200 Loss 0.0298 Accuracy 0.5027\n","Epoch 58 Batch 250 Loss 0.0312 Accuracy 0.5028\n","Epoch 58 Batch 300 Loss 0.0314 Accuracy 0.5028\n","Epoch 58 Loss 0.0317 Accuracy 0.5029\n","Epoch 59 Batch 0 Loss 0.0575 Accuracy 0.4922\n","Epoch 59 Batch 50 Loss 0.0315 Accuracy 0.5031\n","Epoch 59 Batch 100 Loss 0.0294 Accuracy 0.5036\n","Epoch 59 Batch 150 Loss 0.0294 Accuracy 0.5035\n","Epoch 59 Batch 200 Loss 0.0293 Accuracy 0.5035\n","Epoch 59 Batch 250 Loss 0.0294 Accuracy 0.5033\n","Epoch 59 Batch 300 Loss 0.0301 Accuracy 0.5034\n","Epoch 59 Loss 0.0299 Accuracy 0.5034\n","Epoch 60 Batch 0 Loss 0.0017 Accuracy 0.5039\n","Epoch 60 Batch 50 Loss 0.0272 Accuracy 0.5037\n","Epoch 60 Batch 100 Loss 0.0299 Accuracy 0.5034\n","Epoch 60 Batch 150 Loss 0.0295 Accuracy 0.5032\n","Epoch 60 Batch 200 Loss 0.0313 Accuracy 0.5033\n","Epoch 60 Batch 250 Loss 0.0324 Accuracy 0.5033\n","Epoch 60 Batch 300 Loss 0.0335 Accuracy 0.5031\n","Epoch 60 Loss 0.0328 Accuracy 0.5031\n","Epoch 61 Batch 0 Loss 0.0908 Accuracy 0.5039\n","Epoch 61 Batch 50 Loss 0.0297 Accuracy 0.5034\n","Epoch 61 Batch 100 Loss 0.0263 Accuracy 0.5030\n","Epoch 61 Batch 150 Loss 0.0299 Accuracy 0.5029\n","Epoch 61 Batch 200 Loss 0.0304 Accuracy 0.5030\n","Epoch 61 Batch 250 Loss 0.0304 Accuracy 0.5030\n","Epoch 61 Batch 300 Loss 0.0308 Accuracy 0.5031\n","Epoch 61 Loss 0.0309 Accuracy 0.5031\n","Epoch 62 Batch 0 Loss 0.0911 Accuracy 0.5039\n","Epoch 62 Batch 50 Loss 0.0317 Accuracy 0.5031\n","Epoch 62 Batch 100 Loss 0.0310 Accuracy 0.5032\n","Epoch 62 Batch 150 Loss 0.0302 Accuracy 0.5031\n","Epoch 62 Batch 200 Loss 0.0308 Accuracy 0.5029\n","Epoch 62 Batch 250 Loss 0.0320 Accuracy 0.5028\n","Epoch 62 Batch 300 Loss 0.0314 Accuracy 0.5030\n","Epoch 62 Loss 0.0318 Accuracy 0.5030\n","Epoch 63 Batch 0 Loss 0.0343 Accuracy 0.4961\n","Epoch 63 Batch 50 Loss 0.0271 Accuracy 0.5028\n","Epoch 63 Batch 100 Loss 0.0306 Accuracy 0.5029\n","Epoch 63 Batch 150 Loss 0.0334 Accuracy 0.5029\n","Epoch 63 Batch 200 Loss 0.0314 Accuracy 0.5033\n","Epoch 63 Batch 250 Loss 0.0315 Accuracy 0.5031\n","Epoch 63 Batch 300 Loss 0.0318 Accuracy 0.5031\n","Epoch 63 Loss 0.0319 Accuracy 0.5031\n","Epoch 64 Batch 0 Loss 0.0106 Accuracy 0.5000\n","Epoch 64 Batch 50 Loss 0.0544 Accuracy 0.5010\n","Epoch 64 Batch 100 Loss 0.0417 Accuracy 0.5026\n","Epoch 64 Batch 150 Loss 0.0385 Accuracy 0.5029\n","Epoch 64 Batch 200 Loss 0.0367 Accuracy 0.5030\n","Epoch 64 Batch 250 Loss 0.0369 Accuracy 0.5028\n","Epoch 64 Batch 300 Loss 0.0366 Accuracy 0.5029\n","Epoch 64 Loss 0.0365 Accuracy 0.5028\n","Epoch 65 Batch 0 Loss 0.0180 Accuracy 0.5000\n","Epoch 65 Batch 50 Loss 0.0319 Accuracy 0.5031\n","Epoch 65 Batch 100 Loss 0.0304 Accuracy 0.5029\n","Epoch 65 Batch 150 Loss 0.0290 Accuracy 0.5031\n","Epoch 65 Batch 200 Loss 0.0302 Accuracy 0.5030\n","Epoch 65 Batch 250 Loss 0.0322 Accuracy 0.5031\n","Epoch 65 Batch 300 Loss 0.0315 Accuracy 0.5033\n","Epoch 65 Loss 0.0317 Accuracy 0.5032\n","Epoch 66 Batch 0 Loss 0.0668 Accuracy 0.5039\n","Epoch 66 Batch 50 Loss 0.0377 Accuracy 0.5015\n","Epoch 66 Batch 100 Loss 0.0344 Accuracy 0.5020\n","Epoch 66 Batch 150 Loss 0.0337 Accuracy 0.5020\n","Epoch 66 Batch 200 Loss 0.0338 Accuracy 0.5022\n","Epoch 66 Batch 250 Loss 0.0332 Accuracy 0.5026\n","Epoch 66 Batch 300 Loss 0.0328 Accuracy 0.5027\n","Epoch 66 Loss 0.0329 Accuracy 0.5028\n","Epoch 67 Batch 0 Loss 0.0287 Accuracy 0.5039\n","Epoch 67 Batch 50 Loss 0.0321 Accuracy 0.5032\n","Epoch 67 Batch 100 Loss 0.0299 Accuracy 0.5030\n","Epoch 67 Batch 150 Loss 0.0304 Accuracy 0.5030\n","Epoch 67 Batch 200 Loss 0.0289 Accuracy 0.5032\n","Epoch 67 Batch 250 Loss 0.0295 Accuracy 0.5032\n","Epoch 67 Batch 300 Loss 0.0307 Accuracy 0.5030\n","Epoch 67 Loss 0.0304 Accuracy 0.5031\n","Epoch 68 Batch 0 Loss 0.0391 Accuracy 0.5000\n","Epoch 68 Batch 50 Loss 0.0300 Accuracy 0.5037\n","Epoch 68 Batch 100 Loss 0.0276 Accuracy 0.5032\n","Epoch 68 Batch 150 Loss 0.0287 Accuracy 0.5030\n","Epoch 68 Batch 200 Loss 0.0302 Accuracy 0.5030\n","Epoch 68 Batch 250 Loss 0.0300 Accuracy 0.5031\n","Epoch 68 Batch 300 Loss 0.0304 Accuracy 0.5031\n","Epoch 68 Loss 0.0302 Accuracy 0.5032\n","Epoch 69 Batch 0 Loss 0.0321 Accuracy 0.5156\n","Epoch 69 Batch 50 Loss 0.0300 Accuracy 0.5035\n","Epoch 69 Batch 100 Loss 0.0305 Accuracy 0.5035\n","Epoch 69 Batch 150 Loss 0.0314 Accuracy 0.5034\n","Epoch 69 Batch 200 Loss 0.0312 Accuracy 0.5034\n","Epoch 69 Batch 250 Loss 0.0311 Accuracy 0.5032\n","Epoch 69 Batch 300 Loss 0.0317 Accuracy 0.5030\n","Epoch 69 Loss 0.0315 Accuracy 0.5031\n","Epoch 70 Batch 0 Loss 0.0117 Accuracy 0.5039\n","Epoch 70 Batch 50 Loss 0.0315 Accuracy 0.5025\n","Epoch 70 Batch 100 Loss 0.0300 Accuracy 0.5034\n","Epoch 70 Batch 150 Loss 0.0294 Accuracy 0.5032\n","Epoch 70 Batch 200 Loss 0.0303 Accuracy 0.5030\n","Epoch 70 Batch 250 Loss 0.0304 Accuracy 0.5031\n","Epoch 70 Batch 300 Loss 0.0313 Accuracy 0.5031\n","Epoch 70 Loss 0.0311 Accuracy 0.5032\n","Epoch 71 Batch 0 Loss 0.0199 Accuracy 0.5156\n","Epoch 71 Batch 50 Loss 0.0270 Accuracy 0.5034\n","Epoch 71 Batch 100 Loss 0.0284 Accuracy 0.5029\n","Epoch 71 Batch 150 Loss 0.0305 Accuracy 0.5027\n","Epoch 71 Batch 200 Loss 0.0299 Accuracy 0.5033\n","Epoch 71 Batch 250 Loss 0.0301 Accuracy 0.5034\n","Epoch 71 Batch 300 Loss 0.0293 Accuracy 0.5034\n","Epoch 71 Loss 0.0297 Accuracy 0.5034\n","Epoch 72 Batch 0 Loss 0.0189 Accuracy 0.5078\n","Epoch 72 Batch 50 Loss 0.0270 Accuracy 0.5026\n","Epoch 72 Batch 100 Loss 0.0272 Accuracy 0.5032\n","Epoch 72 Batch 150 Loss 0.0292 Accuracy 0.5030\n","Epoch 72 Batch 200 Loss 0.0286 Accuracy 0.5034\n","Epoch 72 Batch 250 Loss 0.0293 Accuracy 0.5033\n","Epoch 72 Batch 300 Loss 0.0295 Accuracy 0.5033\n","Epoch 72 Loss 0.0295 Accuracy 0.5033\n","Epoch 73 Batch 0 Loss 0.0051 Accuracy 0.5078\n","Epoch 73 Batch 50 Loss 0.0241 Accuracy 0.5027\n","Epoch 73 Batch 100 Loss 0.0257 Accuracy 0.5035\n","Epoch 73 Batch 150 Loss 0.0263 Accuracy 0.5032\n","Epoch 73 Batch 200 Loss 0.0277 Accuracy 0.5033\n","Epoch 73 Batch 250 Loss 0.0284 Accuracy 0.5032\n","Epoch 73 Batch 300 Loss 0.0297 Accuracy 0.5031\n","Epoch 73 Loss 0.0299 Accuracy 0.5032\n","Epoch 74 Batch 0 Loss 0.0126 Accuracy 0.5078\n","Epoch 74 Batch 50 Loss 0.0263 Accuracy 0.5028\n","Epoch 74 Batch 100 Loss 0.0312 Accuracy 0.5028\n","Epoch 74 Batch 150 Loss 0.0292 Accuracy 0.5034\n","Epoch 74 Batch 200 Loss 0.0285 Accuracy 0.5037\n","Epoch 74 Batch 250 Loss 0.0276 Accuracy 0.5037\n","Epoch 74 Batch 300 Loss 0.0297 Accuracy 0.5034\n","Epoch 74 Loss 0.0304 Accuracy 0.5033\n","Epoch 75 Batch 0 Loss 0.0238 Accuracy 0.5156\n","Epoch 75 Batch 50 Loss 0.0230 Accuracy 0.5058\n","Epoch 75 Batch 100 Loss 0.0323 Accuracy 0.5038\n","Epoch 75 Batch 150 Loss 0.0308 Accuracy 0.5034\n","Epoch 75 Batch 200 Loss 0.0315 Accuracy 0.5031\n","Epoch 75 Batch 250 Loss 0.0314 Accuracy 0.5030\n","Epoch 75 Batch 300 Loss 0.0312 Accuracy 0.5032\n","Epoch 75 Loss 0.0328 Accuracy 0.5032\n","Epoch 76 Batch 0 Loss 0.0608 Accuracy 0.5195\n","Epoch 76 Batch 50 Loss 0.0294 Accuracy 0.5034\n","Epoch 76 Batch 100 Loss 0.0286 Accuracy 0.5033\n","Epoch 76 Batch 150 Loss 0.0301 Accuracy 0.5030\n","Epoch 76 Batch 200 Loss 0.0292 Accuracy 0.5028\n","Epoch 76 Batch 250 Loss 0.0315 Accuracy 0.5030\n","Epoch 76 Batch 300 Loss 0.0309 Accuracy 0.5031\n","Epoch 76 Loss 0.0307 Accuracy 0.5032\n","Epoch 77 Batch 0 Loss 0.0083 Accuracy 0.5000\n","Epoch 77 Batch 50 Loss 0.0259 Accuracy 0.5026\n","Epoch 77 Batch 100 Loss 0.0261 Accuracy 0.5037\n","Epoch 77 Batch 150 Loss 0.0279 Accuracy 0.5032\n","Epoch 77 Batch 200 Loss 0.0292 Accuracy 0.5033\n","Epoch 77 Batch 250 Loss 0.0288 Accuracy 0.5034\n","Epoch 77 Batch 300 Loss 0.0292 Accuracy 0.5033\n","Epoch 77 Loss 0.0291 Accuracy 0.5032\n","Epoch 78 Batch 0 Loss 0.0052 Accuracy 0.5078\n","Epoch 78 Batch 50 Loss 0.0261 Accuracy 0.5041\n","Epoch 78 Batch 100 Loss 0.0263 Accuracy 0.5048\n","Epoch 78 Batch 150 Loss 0.0264 Accuracy 0.5041\n","Epoch 78 Batch 200 Loss 0.0269 Accuracy 0.5039\n","Epoch 78 Batch 250 Loss 0.0274 Accuracy 0.5037\n","Epoch 78 Batch 300 Loss 0.0277 Accuracy 0.5036\n","Epoch 78 Loss 0.0285 Accuracy 0.5035\n","Epoch 79 Batch 0 Loss 0.0970 Accuracy 0.5078\n","Epoch 79 Batch 50 Loss 0.9225 Accuracy 0.2957\n","Epoch 79 Batch 100 Loss 0.8501 Accuracy 0.2830\n","Epoch 79 Batch 150 Loss 0.7575 Accuracy 0.3096\n","Epoch 79 Batch 200 Loss 0.6444 Accuracy 0.3446\n","Epoch 79 Batch 250 Loss 0.5553 Accuracy 0.3703\n","Epoch 79 Batch 300 Loss 0.4885 Accuracy 0.3890\n","Epoch 79 Loss 0.4764 Accuracy 0.3924\n","Epoch 80 Batch 0 Loss 0.1703 Accuracy 0.4805\n","Epoch 80 Batch 50 Loss 0.1182 Accuracy 0.4867\n","Epoch 80 Batch 100 Loss 0.1166 Accuracy 0.4891\n","Epoch 80 Batch 150 Loss 0.1065 Accuracy 0.4911\n","Epoch 80 Batch 200 Loss 0.1049 Accuracy 0.4920\n","Epoch 80 Batch 250 Loss 0.1044 Accuracy 0.4920\n","Epoch 80 Batch 300 Loss 0.1008 Accuracy 0.4923\n","Epoch 80 Loss 0.0997 Accuracy 0.4926\n","Epoch 81 Batch 0 Loss 0.0320 Accuracy 0.5000\n","Epoch 81 Batch 50 Loss 0.0811 Accuracy 0.4976\n","Epoch 81 Batch 100 Loss 0.0743 Accuracy 0.4985\n","Epoch 81 Batch 150 Loss 0.0729 Accuracy 0.4982\n","Epoch 81 Batch 200 Loss 0.0723 Accuracy 0.4981\n","Epoch 81 Batch 250 Loss 0.0723 Accuracy 0.4979\n","Epoch 81 Batch 300 Loss 0.0715 Accuracy 0.4978\n","Epoch 81 Loss 0.0707 Accuracy 0.4979\n","Epoch 82 Batch 0 Loss 0.0550 Accuracy 0.5000\n","Epoch 82 Batch 50 Loss 0.0646 Accuracy 0.4988\n","Epoch 82 Batch 100 Loss 0.0612 Accuracy 0.4992\n","Epoch 82 Batch 150 Loss 0.0617 Accuracy 0.4991\n","Epoch 82 Batch 200 Loss 0.0605 Accuracy 0.4991\n","Epoch 82 Batch 250 Loss 0.0602 Accuracy 0.4993\n","Epoch 82 Batch 300 Loss 0.0597 Accuracy 0.4994\n","Epoch 82 Loss 0.0595 Accuracy 0.4995\n","Epoch 83 Batch 0 Loss 0.1197 Accuracy 0.5000\n","Epoch 83 Batch 50 Loss 0.0727 Accuracy 0.4979\n","Epoch 83 Batch 100 Loss 0.0608 Accuracy 0.4994\n","Epoch 83 Batch 150 Loss 0.0551 Accuracy 0.5003\n","Epoch 83 Batch 200 Loss 0.0518 Accuracy 0.5006\n","Epoch 83 Batch 250 Loss 0.0521 Accuracy 0.5003\n","Epoch 83 Batch 300 Loss 0.0516 Accuracy 0.5005\n","Epoch 83 Loss 0.0521 Accuracy 0.5004\n","Epoch 84 Batch 0 Loss 0.0192 Accuracy 0.5039\n","Epoch 84 Batch 50 Loss 0.0476 Accuracy 0.5004\n","Epoch 84 Batch 100 Loss 0.0478 Accuracy 0.5005\n","Epoch 84 Batch 150 Loss 0.0459 Accuracy 0.5006\n","Epoch 84 Batch 200 Loss 0.0452 Accuracy 0.5011\n","Epoch 84 Batch 250 Loss 0.0484 Accuracy 0.5008\n","Epoch 84 Batch 300 Loss 0.0495 Accuracy 0.5009\n","Epoch 84 Loss 0.0524 Accuracy 0.5005\n","Epoch 85 Batch 0 Loss 0.0242 Accuracy 0.4961\n","Epoch 85 Batch 50 Loss 0.0673 Accuracy 0.4995\n","Epoch 85 Batch 100 Loss 0.0592 Accuracy 0.5006\n","Epoch 85 Batch 150 Loss 0.0587 Accuracy 0.4999\n","Epoch 85 Batch 200 Loss 0.0601 Accuracy 0.4996\n","Epoch 85 Batch 250 Loss 0.0589 Accuracy 0.4995\n","Epoch 85 Batch 300 Loss 0.0606 Accuracy 0.4994\n","Epoch 85 Loss 0.0606 Accuracy 0.4994\n","Epoch 86 Batch 0 Loss 0.0250 Accuracy 0.4961\n","Epoch 86 Batch 50 Loss 0.0517 Accuracy 0.5015\n","Epoch 86 Batch 100 Loss 0.0474 Accuracy 0.5022\n","Epoch 86 Batch 150 Loss 0.0500 Accuracy 0.5013\n","Epoch 86 Batch 200 Loss 0.0497 Accuracy 0.5013\n","Epoch 86 Batch 250 Loss 0.0503 Accuracy 0.5011\n","Epoch 86 Batch 300 Loss 0.0500 Accuracy 0.5009\n","Epoch 86 Loss 0.0504 Accuracy 0.5009\n","Epoch 87 Batch 0 Loss 0.0237 Accuracy 0.5000\n","Epoch 87 Batch 50 Loss 0.0476 Accuracy 0.5016\n","Epoch 87 Batch 100 Loss 0.0512 Accuracy 0.5010\n","Epoch 87 Batch 150 Loss 0.0544 Accuracy 0.5005\n","Epoch 87 Batch 200 Loss 0.0559 Accuracy 0.5002\n","Epoch 87 Batch 250 Loss 0.0534 Accuracy 0.5002\n","Epoch 87 Batch 300 Loss 0.0531 Accuracy 0.5002\n","Epoch 87 Loss 0.0535 Accuracy 0.5001\n","Epoch 88 Batch 0 Loss 0.0166 Accuracy 0.5039\n","Epoch 88 Batch 50 Loss 0.0451 Accuracy 0.5010\n","Epoch 88 Batch 100 Loss 0.0428 Accuracy 0.5016\n","Epoch 88 Batch 150 Loss 0.0469 Accuracy 0.5012\n","Epoch 88 Batch 200 Loss 0.0462 Accuracy 0.5012\n","Epoch 88 Batch 250 Loss 0.0467 Accuracy 0.5013\n","Epoch 88 Batch 300 Loss 0.0460 Accuracy 0.5015\n","Epoch 88 Loss 0.0456 Accuracy 0.5015\n","Epoch 89 Batch 0 Loss 0.0162 Accuracy 0.5039\n","Epoch 89 Batch 50 Loss 0.0562 Accuracy 0.4995\n","Epoch 89 Batch 100 Loss 0.0460 Accuracy 0.5004\n","Epoch 89 Batch 150 Loss 0.0466 Accuracy 0.5003\n","Epoch 89 Batch 200 Loss 0.0454 Accuracy 0.5005\n","Epoch 89 Batch 250 Loss 0.0494 Accuracy 0.5006\n","Epoch 89 Batch 300 Loss 0.0502 Accuracy 0.5005\n","Epoch 89 Loss 0.0502 Accuracy 0.5006\n","Epoch 90 Batch 0 Loss 0.0226 Accuracy 0.5078\n","Epoch 90 Batch 50 Loss 0.0379 Accuracy 0.5019\n","Epoch 90 Batch 100 Loss 0.0402 Accuracy 0.5018\n","Epoch 90 Batch 150 Loss 0.0392 Accuracy 0.5019\n","Epoch 90 Batch 200 Loss 0.0420 Accuracy 0.5018\n","Epoch 90 Batch 250 Loss 0.0491 Accuracy 0.5013\n","Epoch 90 Batch 300 Loss 0.0481 Accuracy 0.5015\n","Epoch 90 Loss 0.0475 Accuracy 0.5014\n","Epoch 91 Batch 0 Loss 0.1187 Accuracy 0.5000\n","Epoch 91 Batch 50 Loss 0.0472 Accuracy 0.5028\n","Epoch 91 Batch 100 Loss 0.0421 Accuracy 0.5019\n","Epoch 91 Batch 150 Loss 0.0394 Accuracy 0.5016\n","Epoch 91 Batch 200 Loss 0.0423 Accuracy 0.5013\n","Epoch 91 Batch 250 Loss 0.0428 Accuracy 0.5016\n","Epoch 91 Batch 300 Loss 0.0457 Accuracy 0.5015\n","Epoch 91 Loss 0.0458 Accuracy 0.5015\n","Epoch 92 Batch 0 Loss 0.0064 Accuracy 0.5039\n","Epoch 92 Batch 50 Loss 0.0343 Accuracy 0.5031\n","Epoch 92 Batch 100 Loss 0.0397 Accuracy 0.5028\n","Epoch 92 Batch 150 Loss 0.0390 Accuracy 0.5026\n","Epoch 92 Batch 200 Loss 0.0431 Accuracy 0.5017\n","Epoch 92 Batch 250 Loss 0.0439 Accuracy 0.5017\n","Epoch 92 Batch 300 Loss 0.0443 Accuracy 0.5016\n","Epoch 92 Loss 0.0444 Accuracy 0.5017\n","Epoch 93 Batch 0 Loss 0.0088 Accuracy 0.5000\n","Epoch 93 Batch 50 Loss 0.0433 Accuracy 0.5023\n","Epoch 93 Batch 100 Loss 0.0461 Accuracy 0.5018\n","Epoch 93 Batch 150 Loss 0.0426 Accuracy 0.5020\n","Epoch 93 Batch 200 Loss 0.0447 Accuracy 0.5018\n","Epoch 93 Batch 250 Loss 0.0435 Accuracy 0.5019\n","Epoch 93 Batch 300 Loss 0.0478 Accuracy 0.5013\n","Epoch 93 Loss 0.0477 Accuracy 0.5012\n","Epoch 94 Batch 0 Loss 0.0365 Accuracy 0.4961\n","Epoch 94 Batch 50 Loss 0.0401 Accuracy 0.5010\n","Epoch 94 Batch 100 Loss 0.0428 Accuracy 0.5010\n","Epoch 94 Batch 150 Loss 0.0383 Accuracy 0.5017\n","Epoch 94 Batch 200 Loss 0.0385 Accuracy 0.5018\n","Epoch 94 Batch 250 Loss 0.0394 Accuracy 0.5019\n","Epoch 94 Batch 300 Loss 0.0408 Accuracy 0.5017\n","Epoch 94 Loss 0.0417 Accuracy 0.5017\n","Epoch 95 Batch 0 Loss 0.0276 Accuracy 0.5039\n","Epoch 95 Batch 50 Loss 0.0328 Accuracy 0.5033\n","Epoch 95 Batch 100 Loss 0.0396 Accuracy 0.5026\n","Epoch 95 Batch 150 Loss 0.0372 Accuracy 0.5027\n","Epoch 95 Batch 200 Loss 0.0365 Accuracy 0.5027\n","Epoch 95 Batch 250 Loss 0.0388 Accuracy 0.5023\n","Epoch 95 Batch 300 Loss 0.0393 Accuracy 0.5022\n","Epoch 95 Loss 0.0393 Accuracy 0.5023\n","Epoch 96 Batch 0 Loss 0.1143 Accuracy 0.4922\n","Epoch 96 Batch 50 Loss 0.0504 Accuracy 0.5002\n","Epoch 96 Batch 100 Loss 0.0430 Accuracy 0.5016\n","Epoch 96 Batch 150 Loss 0.0427 Accuracy 0.5021\n","Epoch 96 Batch 200 Loss 0.0428 Accuracy 0.5020\n","Epoch 96 Batch 250 Loss 0.0429 Accuracy 0.5021\n","Epoch 96 Batch 300 Loss 0.0437 Accuracy 0.5020\n","Epoch 96 Loss 0.0432 Accuracy 0.5020\n","Epoch 97 Batch 0 Loss 0.0088 Accuracy 0.5039\n","Epoch 97 Batch 50 Loss 0.0328 Accuracy 0.5024\n","Epoch 97 Batch 100 Loss 0.0375 Accuracy 0.5029\n","Epoch 97 Batch 150 Loss 0.0345 Accuracy 0.5029\n","Epoch 97 Batch 200 Loss 0.0414 Accuracy 0.5022\n","Epoch 97 Batch 250 Loss 0.0406 Accuracy 0.5022\n","Epoch 97 Batch 300 Loss 0.0416 Accuracy 0.5019\n","Epoch 97 Loss 0.0420 Accuracy 0.5019\n","Epoch 98 Batch 0 Loss 0.0932 Accuracy 0.4961\n","Epoch 98 Batch 50 Loss 0.0406 Accuracy 0.5020\n","Epoch 98 Batch 100 Loss 0.0385 Accuracy 0.5025\n","Epoch 98 Batch 150 Loss 0.0386 Accuracy 0.5021\n","Epoch 98 Batch 200 Loss 0.0402 Accuracy 0.5018\n","Epoch 98 Batch 250 Loss 0.0401 Accuracy 0.5016\n","Epoch 98 Batch 300 Loss 0.0414 Accuracy 0.5017\n","Epoch 98 Loss 0.0420 Accuracy 0.5017\n","Epoch 99 Batch 0 Loss 0.0079 Accuracy 0.5039\n","Epoch 99 Batch 50 Loss 0.0458 Accuracy 0.5011\n","Epoch 99 Batch 100 Loss 0.0435 Accuracy 0.5017\n","Epoch 99 Batch 150 Loss 0.0411 Accuracy 0.5016\n","Epoch 99 Batch 200 Loss 0.0420 Accuracy 0.5016\n","Epoch 99 Batch 250 Loss 0.0438 Accuracy 0.5016\n","Epoch 99 Batch 300 Loss 0.0440 Accuracy 0.5014\n","Epoch 99 Loss 0.0445 Accuracy 0.5015\n","Epoch 100 Batch 0 Loss 0.0532 Accuracy 0.5117\n","Epoch 100 Batch 50 Loss 0.0445 Accuracy 0.5022\n","Epoch 100 Batch 100 Loss 0.0436 Accuracy 0.5009\n","Epoch 100 Batch 150 Loss 0.0434 Accuracy 0.5010\n","Epoch 100 Batch 200 Loss 0.0450 Accuracy 0.5010\n","Epoch 100 Batch 250 Loss 0.0468 Accuracy 0.5007\n","Epoch 100 Batch 300 Loss 0.0493 Accuracy 0.5007\n","Epoch 100 Loss 0.0491 Accuracy 0.5008\n","CPU times: user 27min 9s, sys: 2min 17s, total: 29min 27s\n","Wall time: 20min 9s\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"a3O6o6-3c9dq","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1606204289426,"user_tz":-180,"elapsed":3049024,"user":{"displayName":"Дмитрий Фукин","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhOgXORSMlpkMpukDZcLijzceS9AaqgT1tfLKD3jA=s64","userId":"04706038827385634385"}},"outputId":"7be76504-7cb4-4d31-d7b8-bb6ed3493c01"},"source":["def evaluate(inp_sentence):\n","    start_token = [1]\n","    end_token = [2]\n","  \n","    sentence = preprocess_sentence(inp_sentence)\n","    inputs = [inp_lang_tokenizer.word_index[i] for i in sentence.split(' ')]\n","    \n","    encoder_input = tf.expand_dims(inputs, 0)\n","  \n","  # as the target is english, the first word to the transformer should be the\n","  # english start token.\n","    decoder_input = [1]\n","    output = tf.expand_dims(decoder_input, 0)\n","    \n","    for i in range(max_length_targ):\n","        enc_padding_mask, combined_mask, dec_padding_mask = create_masks(\n","            encoder_input, output)\n","  \n","    # predictions.shape == (batch_size, seq_len, vocab_size)\n","        predictions, attention_weights = transformer(encoder_input, \n","                                                 output,\n","                                                 False,\n","                                                 enc_padding_mask,\n","                                                 combined_mask,\n","                                                 dec_padding_mask)\n","    \n","    # select the last word from the seq_len dimension\n","        predictions = predictions[: ,-1:, :]  # (batch_size, 1, vocab_size)\n","\n","        predicted_id = tf.cast(tf.argmax(predictions, axis=-1), tf.int32)\n","    \n","    # return the result if the predicted_id is equal to the end token\n","        if predicted_id == targ_lang_tokenizer.word_index[\"<end>\"]:\n","            return tf.squeeze(output, axis=0), attention_weights\n","    \n","    # concatentate the predicted_id to the output which is given to the decoder\n","    # as its input.\n","        output = tf.concat([output, predicted_id], axis=-1)\n","\n","    return tf.squeeze(output, axis=0), attention_weights\n","\n","\n","def plot_attention_weights(attention, sentence, result, layer):\n","    fig = plt.figure(figsize=(16, 8))\n","  \n","    sentence = inp_lang_tokenizer.encode(sentence)\n","  \n","    attention = tf.squeeze(attention[layer], axis=0)\n","  \n","    for head in range(attention.shape[0]):\n","        ax = fig.add_subplot(2, 4, head+1)\n","\n","        # plot the attention weights\n","        ax.matshow(attention[head][:-1, :], cmap='viridis')\n","\n","        fontdict = {'fontsize': 10}\n","\n","        ax.set_xticks(range(len(sentence)+2))\n","        ax.set_yticks(range(len(result)))\n","\n","        ax.set_ylim(len(result)-1.5, -0.5)\n","\n","        ax.set_xticklabels(\n","            ['<start>']+[tokenizer_pt.decode([i]) for i in sentence]+['<end>'], \n","            fontdict=fontdict, rotation=90)\n","\n","        ax.set_yticklabels([tokenizer_en.decode([i]) for i in result \n","                            if i < tokenizer_en.vocab_size], \n","                           fontdict=fontdict)\n","\n","        ax.set_xlabel('Head {}'.format(head+1))\n","  \n","    plt.tight_layout()\n","    plt.show()\n","\n","def translate(sentence, plot=''):\n","    result, attention_weights = evaluate(sentence)\n","    predicted_sentence = ([targ_lang_tokenizer.index_word[i] for i in result.numpy()])  \n","\n","    print('Input: {}'.format(sentence))\n","    print('Predicted translation: {}'.format(predicted_sentence))\n","  \n","    if plot:\n","        plot_attention_weights(attention_weights, sentence, result, plot)\n","        \n","translate(\"good morning.\")"],"execution_count":39,"outputs":[{"output_type":"stream","text":["Input: good morning.\n","Predicted translation: ['<start>', '!', '!', '!', '!', '!']\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"gGoHtjBTgbUD","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1606204290029,"user_tz":-180,"elapsed":3049621,"user":{"displayName":"Дмитрий Фукин","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhOgXORSMlpkMpukDZcLijzceS9AaqgT1tfLKD3jA=s64","userId":"04706038827385634385"}},"outputId":"0362860b-7bca-4ec7-f3ef-98e813da041b"},"source":["translate(u\"how are you\")"],"execution_count":40,"outputs":[{"output_type":"stream","text":["Input: how are you\n","Predicted translation: ['<start>', '.', '.', '.', '.', '.']\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"DPDtecCklzfE"},"source":["**Вывод:** качество перевода низкое\n","\n"]}]}